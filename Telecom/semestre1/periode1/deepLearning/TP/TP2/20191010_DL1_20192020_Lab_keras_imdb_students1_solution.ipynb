{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "name": "20191010_DL1_20192020_Lab_keras_imdb_students1_solution.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "PBFqSEkKqpCN"
      },
      "source": [
        "# Lab Deep Learning/ Recurrent Neural Networks/ in keras\n",
        "\n",
        "## Using Many-to-One for movie rating predicton\n",
        "\n",
        "**Author: geoffroy.peeters@telecom-paris.fr**\n",
        "\n",
        "For any remark or suggestion, please feel free to contact me.\n",
        "\n",
        "## Objective:\n",
        "We will implement two different networks to perform automatic rating (0 or 1) of a movie given the text of its review.\n",
        "We will use the ```imdb``` (internet movie database) dataset.\n",
        "\n",
        "The reviews are already available in the form of indexes that point to a word dictionary: each word is already encoded as an index in the dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QmkCSNaXLqjh"
      },
      "source": [
        "## Import packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AOqjzDwioJj9",
        "colab": {}
      },
      "source": [
        "#for axes: 0= lines, 1= column eg mean(x,axis=1) -> compute the mean over each line\n",
        "#[begrow(included):endrow(excluded) , begcolums(included):endcolumn(excluded)] python slicing\n",
        "import numpy as np\n",
        "from keras.datasets import imdb\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing import sequence\n",
        "from keras.layers import Dense, Activation, Embedding, Dropout, Input, LSTM, Reshape, Lambda, RepeatVector\n",
        "from keras import Model\n",
        "from keras import backend as K\n",
        "\n",
        "colab = True\n",
        "student = True"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "v5Yp4OQVvUtr"
      },
      "source": [
        "## Parameters of the model\n",
        "\n",
        "-  We only consider the ```top_words``` first words in the word dictionary\n",
        "- We truncate/zerp-pad each sequence a length ```max_review_length````"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4C_Pv7rYvRkM",
        "colab": {}
      },
      "source": [
        "top_words = 5000 \n",
        "max_review_length = 100\n",
        "INDEX_FROM = 3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ZsNcRimyLzgP"
      },
      "source": [
        "## Import IMDB data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5Gfe1ex8oN8Q",
        "colab": {}
      },
      "source": [
        "# --- Import the IMDB data and only consider the ``top_words``` most used words\n",
        "if colab==False:\n",
        "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words, index_from=INDEX_FROM)\n",
        "else:\n",
        "    # --- IF USING GOOGLE.COLAB\n",
        "    # --- save np.load\n",
        "    np_load_old = np.load\n",
        "    # --- modify the default parameters of np.load \n",
        "    \n",
        "    #np.load = lambda *a,**k: np_load_old(*a, allow_pickle=True, **k) #remplac√©\n",
        "    np.load.__defaults__ = (None,None,True,'ASCII')\n",
        "    # --- call load_data with allow_pickle implicitly set to true\n",
        "    (X_train, y_train), (X_test, y_test) = imdb.load_data(num_words=top_words, index_from=INDEX_FROM)\n",
        "    # --- restore np.load for future normal usage\n",
        "    np.load = np_load_old\n",
        "    \n",
        "    #at the end of this section, X_train will be an array where each element is a list containing the sequence of word's indexes representing reviews(bac,great,good ect..) for that particular movie"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Ljzdx_z-_HMy",
        "outputId": "ed5d7269-4631-4c36-9062-fd3aeed67893",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(X_train[0])"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UEpzyFNX_HM0",
        "outputId": "db6e3d84-a7da-4de6-f268-815e602a6632",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(X_train[0])"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "iSc5LmksOLyr"
      },
      "source": [
        "## Data content\n",
        "\n",
        "- ```X_train``` and ```X_test``` are numpy arrays of lists. \n",
        "  - each item in a list is the index in the word dictionary. So that a list is the sequence of index of words.\n",
        "\n",
        "- ```y_train``` and ```y_test``` are a numpy arrays of the same dimension as ```X_train``` and ```X_test``` \n",
        "  - they contains the values 0 (bad movie) or 1 (good movie)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WouODCPrtiuu",
        "outputId": "dee2ff3c-e15f-4593-9a33-c63b97241da8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 406
        }
      },
      "source": [
        "print(\"type(X_train):\", type(X_train))\n",
        "#the training input is actually an array where each element is a list(each list contains the words of the an input sequence)\n",
        "print(\"number of training sequences: X_train.shape:\", X_train.shape) \n",
        "print(\"type(X_train[0]):\",type(X_train[0]))\n",
        "print(\"length of the first training sequence: len(X_train[0]):\",len(X_train[0]))\n",
        "print(\"length of the second training sequence: len(X_train[1]):\",len(X_train[1]))\n",
        "print(\"list of data of the first training sequence: X_train[0]:\", X_train[0] )\n",
        "len_list = [len(train) for train in X_train]\n",
        "len_list_test = [len(train) for train in X_test]\n",
        "print(\"maximum length of a training sequence:\", max(len_list))\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.hist(len_list, 100);"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "type(X_train): <class 'numpy.ndarray'>\n",
            "number of training sequences: X_train.shape: (25000,)\n",
            "type(X_train[0]): <class 'list'>\n",
            "length of the first training sequence: len(X_train[0]): 218\n",
            "length of the second training sequence: len(X_train[1]): 189\n",
            "list of data of the first training sequence: X_train[0]: [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 2, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 2, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 2, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 2, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 2, 8, 4, 107, 117, 2, 15, 256, 4, 2, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 2, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 2, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 2, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 2, 19, 178, 32]\n",
            "maximum length of a training sequence: 2494\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAD8CAYAAAB+UHOxAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2I-cEKUh_HM4"
      },
      "source": [
        "## Details of how the reviews are encoded"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XcOwiMUT_HM5",
        "outputId": "9b9cae34-caa2-480c-a234-7072a11caa3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "#word_to_id is a map containig the word as key and the index as value and we skep the first 3 indexes in order to assign them to <PAD>, >START>, and <UNK>\n",
        "word_to_id = imdb.get_word_index()\n",
        "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
        "word_to_id[\"<PAD>\"] = 0\n",
        "word_to_id[\"<START>\"] = 1\n",
        "word_to_id[\"<UNK>\"] = 2\n",
        "\n",
        "#contains the mapping between word and id\n",
        "id_to_word = {value:key for key,value in word_to_id.items()}\n",
        "print(' '.join(id_to_word[id] for id in X_train[1000] ))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<START> although i had seen <UNK> in a theater way back in <UNK> i couldn't remember anything of the plot except for vague images of kurt thomas running and fighting against a backdrop of stone walls and disappointment regarding the ending br br after reading some of the other reviews i picked up a copy of the newly released dvd to once again enter the world of <UNK> br br it turns out this is one of those films produced during the <UNK> that would go directly to video today the film stars <UNK> <UNK> kurt thomas as jonathan <UNK> <UNK> out of the blue to <UNK> the nation of <UNK> to enter and hopefully win the game a <UNK> <UNK> <UNK> by the khan who <UNK> his people by yelling what sounds like <UNK> power the goal of the mission involves the star wars defense system jonathan is trained in the martial arts by princess <UNK> who never speaks or leaves the house once trained tries to blend in with the <UNK> by wearing a bright red <UNK> with <UNK> of blue and white needless to say <UNK> finds himself running and fighting for his life along the stone streets of <UNK> on his way to a date with destiny and the game br br star kurt thomas was ill served by director robert <UNK> who it looks like was never on the set the so called script is just this side of incompetent see other reviews for the many <UNK> throughout the town of <UNK> has a few good moments but is ultimately ruined by bad editing the ending <UNK> still there's the <UNK> of a good action adventure here a hong kong version with more <UNK> action and faster pace might even be pretty good\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Hfl42LGCugWB",
        "outputId": "0806860d-e70a-4815-f532-b89e502556ae",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(\"type(y_train):\", type(y_train))\n",
        "print(\"y_train.shape:\", y_train.shape)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "type(y_train): <class 'numpy.ndarray'>\n",
            "y_train.shape: (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iVw65PNNuobX",
        "outputId": "0868ecf0-2c35-4272-c8a7-1cad7d82608f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "print(\"X_test.shape:\", X_test.shape)\n",
        "print(\"y_test.shape:\", y_test.shape)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_test.shape: (25000,)\n",
            "y_test.shape: (25000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "V18OA7oQNH3c"
      },
      "source": [
        "## Data processing\n",
        "\n",
        "Sequences (represented as a list of values) in ```X_train``` represent the reviews.\n",
        "They can have different length.\n",
        "To train the network we should modify them so that they all have the same length.\n",
        "We do this by:\n",
        "- truncating the ones that are too long\n",
        "- padding-with-zero them the ones that are too short.\n",
        "\n",
        "This is obtained using ```sequence.pad_sequences``` of keras."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JhmiHsOGoRwT",
        "outputId": "aa950afa-c9ee-4545-a9df-5d22f0a1c117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "# --- truncate and pad input sequences\n",
        "\n",
        "if student:\n",
        "    # --- START CODE HERE (01)\n",
        "    #this method will truncate each list in the array so that they have a length of 100 in order to be trained\n",
        "    X_train = sequence.pad_sequences(X_train, maxlen =  max_review_length, dtype = \"float32\", padding = \"pre\", truncating = \"pre\", value = 0)\n",
        "    X_test =  sequence.pad_sequences(X_test, maxlen =   max_review_length, dtype = \"float32\", padding = \"pre\", truncating = \"pre\", value = 0)\n",
        "    # --- END CODE HERE\n",
        "\n",
        "print(\"len(X_train[0]):\", len(X_train[0]))\n",
        "print(\"len(X_train[1]):\", len(X_train[1]))\n",
        "print(\"X_train[0]:\", X_train[0])"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "len(X_train[0]): 100\n",
            "len(X_train[1]): 100\n",
            "X_train[0]: [1.415e+03 3.300e+01 6.000e+00 2.200e+01 1.200e+01 2.150e+02 2.800e+01\n",
            " 7.700e+01 5.200e+01 5.000e+00 1.400e+01 4.070e+02 1.600e+01 8.200e+01\n",
            " 2.000e+00 8.000e+00 4.000e+00 1.070e+02 1.170e+02 2.000e+00 1.500e+01\n",
            " 2.560e+02 4.000e+00 2.000e+00 7.000e+00 3.766e+03 5.000e+00 7.230e+02\n",
            " 3.600e+01 7.100e+01 4.300e+01 5.300e+02 4.760e+02 2.600e+01 4.000e+02\n",
            " 3.170e+02 4.600e+01 7.000e+00 4.000e+00 2.000e+00 1.029e+03 1.300e+01\n",
            " 1.040e+02 8.800e+01 4.000e+00 3.810e+02 1.500e+01 2.970e+02 9.800e+01\n",
            " 3.200e+01 2.071e+03 5.600e+01 2.600e+01 1.410e+02 6.000e+00 1.940e+02\n",
            " 2.000e+00 1.800e+01 4.000e+00 2.260e+02 2.200e+01 2.100e+01 1.340e+02\n",
            " 4.760e+02 2.600e+01 4.800e+02 5.000e+00 1.440e+02 3.000e+01 2.000e+00\n",
            " 1.800e+01 5.100e+01 3.600e+01 2.800e+01 2.240e+02 9.200e+01 2.500e+01\n",
            " 1.040e+02 4.000e+00 2.260e+02 6.500e+01 1.600e+01 3.800e+01 1.334e+03\n",
            " 8.800e+01 1.200e+01 1.600e+01 2.830e+02 5.000e+00 1.600e+01 4.472e+03\n",
            " 1.130e+02 1.030e+02 3.200e+01 1.500e+01 1.600e+01 2.000e+00 1.900e+01\n",
            " 1.780e+02 3.200e+01]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "YlrDTuk5K65Q"
      },
      "source": [
        "# First model\n",
        "\n",
        "In the first model, we will simply \n",
        "- learn a word embedding  (```Embedding``` layer in keras) and apply it to each item of the sequence, \n",
        "  -  in keras, embedding is not a matrix going from one-hot-encoding to embedding, but is a layer that goes from index-in-word-dictionary to embedding\n",
        "  - the embedding goes from ```top_words``` dimensions to  ```embedding_vector_length``` dimensions\n",
        "- average the embedding obtained for each word of a sequence over all words of the sequence (you should use ```K.mean``` and ```Lambda``` from the keras backend)\n",
        "- apply a fully connected (```Dense``` layer in keras) which output activation is a sigmoid (predicting the 0 or 1 rating)\n",
        "\n",
        "We will code this model \n",
        "- First, using the Sequential API of keras (https://keras.io/models/sequential/)\n",
        "- Secondly, using the Functional API of keras (https://keras.io/getting-started/functional-api-guide/)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ufW00TGcs3Jj",
        "colab": {}
      },
      "source": [
        "K.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zspaUptgtW9l",
        "outputId": "3ed511ca-0e97-4c28-fb61-8cdc3ad58153",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        }
      },
      "source": [
        "# --- create the model\n",
        "embedding_vector_length = 32\n",
        "\n",
        "# CODE-RNN1-2\n",
        "if student:\n",
        "    # --- START CODE HERE (02)\n",
        "    # --- Using the Sequential API\n",
        "    \n",
        "    #till now we are using a sort one hot encoding where each word is represented by an index\n",
        "    model = Sequential()\n",
        "    #this layer takes as input a matrix of size input_dim*input_length and is able to to return a word embedding matrix of size input_dim*output_dim\n",
        "    model.add(Embedding(input_dim = top_words, output_dim = embedding_vector_length,input_length = max_review_length))\n",
        "    model.add(Lambda(lambda x: K.mean(x,axis =1) ))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "    # --- END CODE HERE\n",
        "    \n",
        "    # --- START CODE HERE (03) just repeat the samething for model 2\n",
        "    # --- Using the Functional API\n",
        "    #NB: when using the input layer shape specify the shape of a single input in the format (seq_len,nLines) whcich is also the format used by model.summary\n",
        "    inputs = Input(shape=(max_review_length,))\n",
        "\n",
        "   # a layer instance is callable on a tensor, and returns a tensor\n",
        "    output_1 =Embedding(input_dim = top_words, output_dim = embedding_vector_length,input_length = max_review_length)(inputs)\n",
        "    output_2 =Lambda(lambda x : K.mean(x,axis =1))(output_1)\n",
        "    outputs =Dense(1,activation= \"sigmoid\")(output_2)\n",
        "    #outputs = Activation('sigmoid',name = \"activation_layer\")(output_3)\n",
        "    model_functional = Model(inputs = inputs, outputs = outputs)\n",
        "    #model = \n",
        "    # --- END CODE HERE\n",
        "    \n",
        "\n",
        "#the trainable parameters in the case of an RNN are the ones in the matrices W,Wax,wya\n",
        "print(model.summary())\n",
        "print(model_functional.summary())"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lambda_1 (Lambda)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 100, 32)           160000    \n",
            "_________________________________________________________________\n",
            "lambda_2 (Lambda)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 160,033\n",
            "Trainable params: 160,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pFXz4AS6tawQ",
        "outputId": "b105ffb2-6330-4366-8562-fa18835acedb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 806
        }
      },
      "source": [
        "#??? the loss is increasing and the accuracy is going down for the first model\n",
        "# --- compile and fit the model 1\n",
        "print(\"training model 1..........\\n\")\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n",
        "\n",
        "print(\"training model........\\n \")\n",
        "# --- compile and fit the model 2\n",
        "model_functional.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_functional.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "training model 1..........\n",
            "\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 2s 98us/step - loss: 0.6401 - acc: 0.7224 - val_loss: 0.5639 - val_acc: 0.7773\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 2s 89us/step - loss: 0.4876 - acc: 0.8154 - val_loss: 0.4405 - val_acc: 0.8186\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 2s 91us/step - loss: 0.3945 - acc: 0.8445 - val_loss: 0.3850 - val_acc: 0.8368\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 2s 92us/step - loss: 0.3472 - acc: 0.8602 - val_loss: 0.3587 - val_acc: 0.8446\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 2s 90us/step - loss: 0.3191 - acc: 0.8695 - val_loss: 0.3452 - val_acc: 0.8489\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 2s 92us/step - loss: 0.2996 - acc: 0.8773 - val_loss: 0.3376 - val_acc: 0.8520\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 2s 90us/step - loss: 0.2845 - acc: 0.8843 - val_loss: 0.3345 - val_acc: 0.8543\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 2s 92us/step - loss: 0.2730 - acc: 0.8893 - val_loss: 0.3336 - val_acc: 0.8548\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 2s 91us/step - loss: 0.2635 - acc: 0.8931 - val_loss: 0.3350 - val_acc: 0.8554\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 2s 90us/step - loss: 0.2556 - acc: 0.8964 - val_loss: 0.3368 - val_acc: 0.8544\n",
            "training model........\n",
            " \n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/10\n",
            "25000/25000 [==============================] - 2s 100us/step - loss: 0.6387 - acc: 0.7329 - val_loss: 0.5630 - val_acc: 0.7737\n",
            "Epoch 2/10\n",
            "25000/25000 [==============================] - 2s 88us/step - loss: 0.4875 - acc: 0.8123 - val_loss: 0.4403 - val_acc: 0.8232\n",
            "Epoch 3/10\n",
            "25000/25000 [==============================] - 2s 88us/step - loss: 0.3944 - acc: 0.8443 - val_loss: 0.3860 - val_acc: 0.8342\n",
            "Epoch 4/10\n",
            "25000/25000 [==============================] - 2s 90us/step - loss: 0.3476 - acc: 0.8600 - val_loss: 0.3592 - val_acc: 0.8437\n",
            "Epoch 5/10\n",
            "25000/25000 [==============================] - 2s 98us/step - loss: 0.3189 - acc: 0.8709 - val_loss: 0.3454 - val_acc: 0.8488\n",
            "Epoch 6/10\n",
            "25000/25000 [==============================] - 2s 97us/step - loss: 0.2990 - acc: 0.8777 - val_loss: 0.3377 - val_acc: 0.8514\n",
            "Epoch 7/10\n",
            "25000/25000 [==============================] - 2s 97us/step - loss: 0.2843 - acc: 0.8839 - val_loss: 0.3343 - val_acc: 0.8540\n",
            "Epoch 8/10\n",
            "25000/25000 [==============================] - 2s 96us/step - loss: 0.2731 - acc: 0.8884 - val_loss: 0.3343 - val_acc: 0.8546\n",
            "Epoch 9/10\n",
            "25000/25000 [==============================] - 2s 91us/step - loss: 0.2635 - acc: 0.8930 - val_loss: 0.3343 - val_acc: 0.8543\n",
            "Epoch 10/10\n",
            "25000/25000 [==============================] - 2s 93us/step - loss: 0.2555 - acc: 0.8964 - val_loss: 0.3367 - val_acc: 0.8541\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f49772a2898>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 100
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SBqyzLJRUIsC"
      },
      "source": [
        "## Results\n",
        "\n",
        "After only 3 epochs, you should obtain an accuracy around 84% for the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nCALyP-Q_HNH",
        "outputId": "3ffcf3f2-6c45-4952-8803-15c6c3b75ab6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "# --- Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy for model 1: %.2f%%\" % (scores[1]*100))\n",
        "\n",
        "scores_functional_model = model_functional.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy for model 2: %.2f%%\" % (scores_functional_model[1]*100))"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy for model 1: 85.44%\n",
            "Accuracy for model 2: 85.41%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uRP-h4Xr_HNJ"
      },
      "source": [
        "## Using the trained embedding to find equivalence between words\n",
        "\n",
        "Since the embedding is part of the models, we can look at the trained embedding matrix $E$ and use it to get the most similar words (according to the trained matrix $E$) in the dictionary.\n",
        "Use the weights of the ```Embedding``` layer to find the most similar words to ```great```. We will use an Euclidean distance for that.\n",
        "- Retrieve the weights of the ```Embedding layer```\n",
        "- Get the position of ```great``` in the dictionary\n",
        "- Get the word-embedding of ```great```\n",
        "- Find (using Euclidean distance), the closest embedded-words to ```great```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7xMubRqJ_HNJ",
        "outputId": "394857f1-2799-4fd1-dd37-e3b869bc838d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if student:\n",
        "    # print(model.parameters())\n",
        "    # --- START CODE HERE (04)\n",
        "    #get the weigths of a specific layer, in this case the embedding layer which is the first one\n",
        "    E = model.get_weights()[0]\n",
        "    \n",
        "    position = word_to_id[\"great\"]\n",
        "    #convert great in one hot vector(of dimession 5000*1) and then get it's one hot encoding by multiplying it with the E(5000*32)\n",
        "    x = np.zeros((5000,1), dtype = np.int)\n",
        "    x[position] = 1\n",
        "    word_embedding_great = np.dot(np.transpose(E),x)\n",
        "    dist = np.linalg.norm(E[position:position+1] - E, axis=1)\n",
        "    #print(\"word embedding of great \\n\",word_embedding_great)\n",
        "    #find the index of the closes word in the norm array\n",
        "    closest_word_index = np.where(dist ==  np.min(dist[np.nonzero(dist)]))[0][0]\n",
        "    #convert it into a one hot encoding\n",
        "    x = np.zeros((5000,1), dtype = np.int)\n",
        "    x[position] = 1\n",
        "   \n",
        "    print(\"the closest word to great is :\",id_to_word[closest_word_index])\n",
        "    #print(\"\\n the word embedding for that closest word is :\\n\")\n",
        "    #print(np.dot(np.transpose(E),x))\n",
        "   \n",
        "    \n",
        "    \n",
        "    # --- END CODE HERE\n"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "the closest word to great is : fantastic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zK9e5Eo1Ks2a"
      },
      "source": [
        "# Second model\n",
        "\n",
        "In the second model, we will replace\n",
        "- the average over the sequence of the obtained embedding\n",
        "- by a RNN layer (more precisely an ```LSTM```) in a Many-To-One configuration with $n_a=100$\n",
        "\n",
        "We will code this model \n",
        "- First, using the Sequential API of keras (https://keras.io/models/sequential/)\n",
        "- Secondly, using the Functional API of keras (https://keras.io/getting-started/functional-api-guide/)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rwoXuOqqVDOy",
        "colab": {}
      },
      "source": [
        "K.clear_session()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7dl-CSMKoViX",
        "outputId": "f04e22ce-29a2-4963-f061-be58f97c94ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        }
      },
      "source": [
        "# --- create the model\n",
        "\n",
        "if student:\n",
        "    # --- START CODE HERE (05)\n",
        "    # --- Using the Sequential API\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(input_dim = top_words, output_dim = embedding_vector_length,input_length = max_review_length))\n",
        "    model.add(LSTM(units = 100,return_sequences = False,name = \"RNN1\"))\n",
        "    # model.add(Lambda(lambda x: K.mean(x,axis =1) ))\n",
        "    model.add(Dense(1, activation='sigmoid'))\n",
        "    # --- END CODE HERE\n",
        "    \n",
        "    # --- START CODE HERE (06)\n",
        "    inputs = Input(shape=(max_review_length,))\n",
        "\n",
        "    # a layer instance is callable on a tensor, and returns a tensor\n",
        "    output_1 =Embedding(input_dim = top_words, output_dim = embedding_vector_length,input_length = max_review_length)(inputs)\n",
        "    # output_2 =Lambda(lambda x : K.mean(x,axis =1))(output_1)\n",
        "    output_2 = LSTM(units=100,return_sequences = False,name = \"RNN1\")(output_1)\n",
        "    output_3 =Dense(1)(output_2)\n",
        "    outputs = Activation('sigmoid',name = \"activation_layer\")(output_3)\n",
        "    model_functional = Model(inputs = inputs, outputs = outputs)\n",
        "    #model = \n",
        "    # --- END CODE HERE\n",
        "   \n",
        "print(model_functional.summary())\n",
        "print(model.summary())"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_1 (InputLayer)         (None, 100)               0         \n",
            "_________________________________________________________________\n",
            "embedding_2 (Embedding)      (None, 100, 32)           160000    \n",
            "_________________________________________________________________\n",
            "RNN1 (LSTM)                  (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1)                 101       \n",
            "_________________________________________________________________\n",
            "activation_layer (Activation (None, 1)                 0         \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (None, 100, 32)           160000    \n",
            "_________________________________________________________________\n",
            "RNN1 (LSTM)                  (None, 100)               53200     \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 1)                 101       \n",
            "=================================================================\n",
            "Total params: 213,301\n",
            "Trainable params: 213,301\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "None\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-bp7PzX7oXtB",
        "outputId": "df04846b-d2c2-41ac-8c7a-4474d572de92",
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "# --- compile and fit the model\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))\n",
        "\n",
        "# --- compile and fit the model 2(functional)\n",
        "model_functional.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model_functional.fit(X_train, y_train, epochs=3, batch_size=64, validation_data=(X_test, y_test))"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 67s 3ms/step - loss: 0.4421 - acc: 0.7866 - val_loss: 0.3402 - val_acc: 0.8499\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 66s 3ms/step - loss: 0.3085 - acc: 0.8710 - val_loss: 0.3458 - val_acc: 0.8506\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 66s 3ms/step - loss: 0.2665 - acc: 0.8908 - val_loss: 0.3473 - val_acc: 0.8478\n",
            "Train on 25000 samples, validate on 25000 samples\n",
            "Epoch 1/3\n",
            "25000/25000 [==============================] - 67s 3ms/step - loss: 0.4396 - acc: 0.7860 - val_loss: 0.3592 - val_acc: 0.8426\n",
            "Epoch 2/3\n",
            "25000/25000 [==============================] - 66s 3ms/step - loss: 0.3089 - acc: 0.8725 - val_loss: 0.3596 - val_acc: 0.8436\n",
            "Epoch 3/3\n",
            "25000/25000 [==============================] - 66s 3ms/step - loss: 0.2706 - acc: 0.8916 - val_loss: 0.3587 - val_acc: 0.8500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f497783c358>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F1LN_fjMWBHJ"
      },
      "source": [
        "## Results\n",
        "\n",
        "After only 3 epochs, you should obtain an accuracy around 88% for the test data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RlMEKRbzoavm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "011b4867-fffb-48ac-9514-93f41fd30cfd"
      },
      "source": [
        "# --- Final evaluation of the model\n",
        "scores = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Accuracy: %.2f%%\" % (scores[1]*100)) "
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 84.78%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LVK5sGgF_HNX"
      },
      "source": [
        "# Evaluation\n",
        "\n",
        "To evaluate the work, you should rate the code for \n",
        "- 1) Data Pre-Processing (01)\n",
        "- 2) First model using the Sequential API (02)\n",
        "- 3) First model using the Functional API (03)\n",
        "- 4) Find equivalence between words (04)\n",
        "- 5) Second model using the Sequential API (05)\n",
        "- 6) Second model using the Functional API (06)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NHUP4IA-_HNY",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}