{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP Coding Convolutional Neural Networks in tensorflow and keras\n",
    "\n",
    "TP author : Geoffroy Peeters, Alasdair Newson\n",
    "\n",
    "### Objective:\n",
    "\n",
    "We want to implement a Convolutional Neural Network (CNN) to do image recognition. For this we will use the well-known CIFAR-10 dataset https://www.cs.toronto.edu/~kriz/cifar.html.\n",
    "\n",
    "The CIFAR-10 dataset consists of 60000 32x32 colour images in 10 classes, with 6000 images per class. There are 50000 training images and 10000 test images.\n",
    "\n",
    "We will first code the simple ConvNet described below using \n",
    "- tensorflow https://www.tensorflow.org\n",
    "\n",
    "then do the same using\n",
    "- keras : https://keras.io\n",
    "\n",
    "\n",
    "The input of the CNN is a set of (32,32,3) image tensors. We apply :\n",
    "\n",
    "    - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same' (i.e. we do not apply zero-padding)\n",
    "    - a ReLu activation function\n",
    "    \n",
    "    - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same' (i.e. we do not apply zero-padding)\n",
    "    - a ReLu activation function\n",
    "    - a Max Pooling Layer of shape (2,2) and stride (2,2) (i.e. we reduce by two the size in each dimension)\n",
    "    \n",
    "    - a Convolutional layer of 32 filters of shape (3,3), with stride (1,1) and padding='same' (i.e. we do not apply zero-padding)\n",
    "    - a ReLu activation function\n",
    "    - a Max Pooling Layer of shape (2,2) and stride (2,2) (i.e. we reduce by two the size in each dimension)\n",
    "    \n",
    "    - We then Flatten the data (reduce them to a vector in order to be able to apply a Fully-Connected layer to it)\n",
    "    - A softmax activation function which outputs are the $P(y_c | X)$ (multi-class problem)\n",
    "\n",
    "### Your task:\n",
    "You need to add the missing parts in the code (parts between # --- START CODE HERE and # --- END CODE HERE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data\n",
    "\n",
    "We first import CIFAR-10 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.datasets import cifar10\n",
    "(X_train, y_train), (X_test, y_test) = cifar10.load_data()\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CIFAR-10 dataset has 10 classes. These are the following :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_10_list = [ 'airplane', 'automobile','bird','cat','deer','dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display some of the images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for idx,i in enumerate(range(100,110)):\n",
    "    plt.subplot(2, 5, idx+1)\n",
    "    plt.imshow(X_train[i, :, :, :])\n",
    "    plt.title(cifar_10_list[int(y_train[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Format conversion\n",
    "\n",
    "We now convert the format of the data. In the case of CNNs, and contrary to MLPs, the inputs are not simply vectors of arbitrary size. For example, in the case of images, it is important to distinguish the different dimensions (otherwise the network would not know how to carry out convolutions). Therefore, you need to carry out the following operation :\n",
    "\n",
    "- reshape the input ```X_train```and ```X_test``` to a set of matrices of size (32,32) and depth 3 (for the 3 R,G,B colors), convert the data to float32 and normalize them in the range [0,1]. The final dimensions of the image data should be : [batch_size, height, width, n_channels]\n",
    "\n",
    "Furthermore, as in the previous lab work, you must also convert the label vectors to matrices with k columns, where k is the number of classes. This allows us to carry out the cross-entropy for multi-class problems. Therefore :\n",
    "\n",
    "- convert the output label ```y_train``` and ```y_test``` to one-hot encoding format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input image dimensions\n",
    "img_rows, img_cols, nb_channels = 32, 32, 3\n",
    "nb_classes = 10\n",
    "\n",
    "# In case depth is 1 (black and white pictures) -> reshape to proper format\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, nb_channels)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, nb_channels)\n",
    "\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "Y_train = to_categorical(y_train)\n",
    "Y_test = to_categorical(y_test)\n",
    "\n",
    "print('X_train shape:', X_train.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('Y_train shape:', Y_train.shape)\n",
    "print('There is {} train data'.format(X_train.shape[0]))\n",
    "print('There is {} test data'.format(X_test.shape[0]))\n",
    "print(img_rows, img_cols, nb_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For memory and speed reasons, we are going to reduce the amount of training data :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_samples = 10000\n",
    "X_train = X_train[0:n_train_samples,:,:,:]\n",
    "Y_train = Y_train[0:n_train_samples,:]\n",
    "print(X_train.shape)\n",
    "print(Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CNN with tensorflow\n",
    "\n",
    "We are now going to create a CNN with Tensorflow, and train it on the previously loaded CIFAR dataset.\n",
    "- First define the input and output using ```tf.placeholder```\n",
    "- Define the weights W1,b1,W2,b2 of the CNN using ```tf.variable``` or ```tf.get_variable```\n",
    "    - You can initialize them using ```tf.contrib.layers.xavier_initializer```(see https://www.tensorflow.org/api_docs/python/tf/contrib/layers/xavier_initializer)\n",
    "- Perform the convolution using ```tf.nn.conv2d``` \n",
    "- Apply the activation function using ```tf.nn.relu```\n",
    "- Perform the max pooling using ```tf.nn.max_pool```\n",
    "- Flatten the output of the convolution using ```tf.contrib.layers.flatten```\n",
    "- Perform the Fully-Connected part using ```tf.contrib.layers.fully_connected```\n",
    "\n",
    "Also, note that in Tensorflow, you can carry out the softmax + cross entropy all in one step with the following function :\n",
    "\n",
    "- ```tf.nn.softmax_cross_entropy_with_logits()```\n",
    "\n",
    "This has the advantage of being optimised and stabilised by the programmers who wrote Tensorflow, so that you do not have numerical problems.\n",
    "\n",
    "$\\textbf{IMPORTANT NOTE}$ : Please note that this part in Tensorflow is just so that you have had some experience with CNNs in Tensorflow. So do not spend a lot of time trying to get great accuracy results. If you want to do this, do it in the Keras part."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "learning_rate = 0.01\n",
    "n_epochs = 20\n",
    "batch_size = 64\n",
    "\n",
    "# number of convolutional filters to use\n",
    "nb_filters = 32\n",
    "# convolution kernel size\n",
    "kernel_size = (3, 3)\n",
    "# size of pooling area for max pooling\n",
    "pool_size = (2, 2)\n",
    "\n",
    "\n",
    "# --- Size of the successice layers\n",
    "n_h_0 = nb_channels\n",
    "n_h_1 = nb_filters\n",
    "n_h_2 = nb_filters\n",
    "n_h_3 = nb_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# input tensor are of shape [batch, in_height, in_width, in_channels]  \n",
    "# filter / kernel tensor are of shape [filter_height, filter_width, in_channels, out_channels]\n",
    "\n",
    "# --- START CODE HERE\n",
    "Z4 = ...\n",
    "# --- END CODE HERE\n",
    "\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits = Z4, labels = Y_output))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "n_batches = int(np.ceil(X_train.shape[0] / float(batch_size)))\n",
    "costs = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        minibatch_cost = 0.\n",
    "        for batch_idx in range(n_batches):\n",
    "            # BEGIN CODE HERE\n",
    "            _ , temp_cost = ...\n",
    "            # END CODE HERE\n",
    "            minibatch_cost += temp_cost / n_batches\n",
    "\n",
    "        if epoch % 1 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" % (epoch, minibatch_cost))\n",
    "            \n",
    "        costs.append(minibatch_cost) \n",
    "        \n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    # Calculate the correct predictions\n",
    "    predict_op = tf.argmax(Z4, 1)\n",
    "    correct_prediction = tf.equal(predict_op, tf.argmax(Y_output, 1))\n",
    "\n",
    "    # Calculate accuracy on the test set\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, \"float\"))\n",
    "    print(accuracy)\n",
    "    train_accuracy = accuracy.eval({X_input: X_train, Y_output: Y_train})\n",
    "    test_accuracy = accuracy.eval({X_input: X_test, Y_output: Y_test})\n",
    "    print(\"Train Accuracy:\", train_accuracy)\n",
    "    print(\"Test Accuracy:\", test_accuracy)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The train accuracy after 20 epochs should be around 0.5. You do not have to push the training further than this, just make sure that the loss decreases and that the accuracy is reasonable. We will now create the same network in Keras."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. CNN with keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are now going to create the same network with Keras. As we shall see, the interface is quite simplified in comparison to Tensorflow.\n",
    "\n",
    "## We first import keras packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras import optimizers\n",
    "print(keras.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We define the parameters of the model, and of the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (img_rows, img_cols, nb_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the CNN model in Keras using the Sequential API\n",
    "\n",
    "Keras has a simple way of adding layers to create a neural network. First, you can indicate to Keras that the model is 'sequential', that is to say, a straight-forward CNN. For this, you can use the following function :\n",
    "- ```model = Sequential()```\n",
    "\n",
    "After this, you can add layers with the function.\n",
    "\n",
    "- ```model.add()```\n",
    "\n",
    "You can then use the ```Conv2D```, ```Activation```, ```MaxPooling2D```, ```Flatten``` and ```Dense``` (fully connected) functions to specify different layer types. Note that in the case of this approach, you will have to specify the input image size in the first layer of the network. So, for example, if the first layer is convolutional :\n",
    "\n",
    "- model = Sequential()\n",
    "- model.add(Conv2D(nb_filters, kernel_size, input_shape=input_shape, name='Conv1'))\n",
    "\n",
    "\n",
    "## Creating the CNN model in Keras using the standard API\n",
    "\n",
    "Otherwise, another approach to creating the model is to explicitly create the input variable, and just cascade the different functions, as in Tensorflow. So, for the same example, we would have :\n",
    "\n",
    "- input = Input(shape=(img_rows,img_cols,nb_channels))\n",
    "- output = Conv2D(input_shape=self.img_shape,filters=nb_filters,kernel_size=kernel_size)(input)\n",
    "- model = Model(input, output)\n",
    "\n",
    "Create your CNN now with the network parameters specified above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- START CODE HERE\n",
    "\n",
    "# --- END CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile the model\n",
    "\n",
    "We define here the ```loss``` that we will minimize and the ```optimizer``` (the specific algorithm used to perform gradient descent)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=optimizers.Adam(lr=learning_rate),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We display a summary of the model created (but not yet trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, Y_train,\n",
    "          batch_size=batch_size, \n",
    "          nb_epoch=n_epochs,\n",
    "          verbose=1, \n",
    "          validation_data=(X_test, Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We evaluate the performances of the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, Y_test, verbose=False)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a look at what the network has learned. What do you think ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for idx,i in enumerate(range(100,110)):\n",
    "    plt.subplot(2, 5, idx+1)\n",
    "    rand_ind = np.random.randint(0,X_test.shape[0])\n",
    "    predicted_class = np.argmax(np.squeeze(model.predict(np.expand_dims(X_test[rand_ind,:,:,:],axis=0))))\n",
    "    plt.imshow(X_test[rand_ind,:,:,:])\n",
    "    plt.title(cifar_10_list[int(predicted_class)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, display all (32) trained filters of the first layer\n",
    "\n",
    "You can explore the network parameters easily with Keras. For example ```model.layers``` is a list of the layers of the network. Each element of the network contains the information necessary for this layer. To show the content of a layer i, type :\n",
    "\n",
    "```dir(model.layers[i])```\n",
    "\n",
    "Display all (32) trained filters of the first layer :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.layers[1].filters)\n",
    "print(model.layers[1].kernel_size)\n",
    "print(model.layers[1].get_weights()[0].shape)\n",
    "plt.figure(figsize=(10, 6))\n",
    "for num in range(0,32):\n",
    "    plt.subplot(8, 4, num+1)\n",
    "    # --- START CODE HERE\n",
    "    # --- END CODE HERE    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation\n",
    "\n",
    "To evaluate the work, you should rate the code for \n",
    "- 1) Tensorflow, declaration of inputs and weights : 2 points\n",
    "- 2) Tensorflow, declaration of model : 2 points\n",
    "- 3) Tensorflow, training : 2 points\n",
    "- 4) Keras : model : 2 points\n",
    "- 5) Keras : displaying weights : 2 points\n",
    "\n",
    "Each correct answer (correct formula/code and code runs) gives 2 points. Total over 10 points."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
