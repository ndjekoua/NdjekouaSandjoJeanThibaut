{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7mI-_EvHopCX"
   },
   "source": [
    "# Lab Deep Learning/ Multi-Layer Perceptron for classification/ in python \n",
    "\n",
    "**Author: geoffroy.peeters@telecom-paris.fr**\n",
    "\n",
    "For any remark or suggestion, please feel free to contact me.\n",
    "\n",
    "$\\newcommand{\\underbr}[2]{\\underbrace{#1}_{\\scriptscriptstyle{#2}}}$\n",
    "\n",
    "## Objective:\n",
    "We want to implement a two layers Multi-Layer Perceptron (MLP) with 1 hidden layer in Python, for a classification problem.\n",
    "\n",
    "The output of the network is simply the output of several cascaded functions :\n",
    "- Linear transformations. We note the weights of a linear transformation with $W$\n",
    "- Additive biases. We note the parameters of additive biases  with $b$\n",
    "- Non-linearities.\n",
    "\n",
    "For this, we will implement:\n",
    "- the forward propagation\n",
    "- the computation of the cost/loss\n",
    "- the backward propagation (to obtain the gradients)\n",
    "- the update of the parameters\n",
    "\n",
    "Furthermore, we define the following sizes :\n",
    "\n",
    "- $n^{[0]}$ : number of input neurons\n",
    "- $n^{[1]}$ : number of neurons in hidden layer\n",
    "- $n^{[2]}$ : number of neurons in output layer\n",
    "- $m$ : number of training datapoints\n",
    "\n",
    "### Cost function \n",
    "\n",
    "The **cost** is the average of the the **loss** over the training data. Since we are dealing with a binary classification problem, we will use the binary cross-entropy.\n",
    "\n",
    "$\\mathcal{L} = - \\left( y \\log(\\hat{y}) + (1-y) \\log(1-\\hat{y}) \\right),$\n",
    "\n",
    "where \n",
    "- the $y$ are the ground-truth labels of the data and \n",
    "- the $\\hat{y}$ the estimated labels (outputs of the network).\n",
    "\n",
    "### Forward propagation\n",
    "\n",
    "- $\\large \\underbr{Z^{[1]}}{(m,n^{[1]})} = \\underbr{X}{(m,n^{[0]})} \\underbr{W^{[1]}}{(n^{[0]},n^{[1]})}  + \\underbr{b^{[1]}}{n^{(1)}} $\n",
    "- $\\large \\underbr{A^{[1]}}{(m,n^{[1]})} = f(Z^{[1]})$\n",
    "- $\\large \\underbr{Z^{[2]}}{(m,n^{[2]})} = \\underbr{A^{[1]}}{(m,n^{[1]})} \\underbr{W^{[2]}}{(n^{[1]},n^{[2]})}  + \\underbr{b^{[2]}}{n^{(2)}}$\n",
    "- $\\large \\underbr{A^{[2]}}{(m,n^{[2]})} = \\sigma(Z^{[2]})$\n",
    "\n",
    "where \n",
    "- $f$ is a ```Relu``` function (the code is provided)\n",
    "- $\\sigma$ is a sigmoid function (the code is provided)\n",
    "\n",
    "### Backward propagation\n",
    "\n",
    "The backward propagation can be calculated as\n",
    "\n",
    "- $\\large \\underbr{dZ^{[2]}}{(m,n^{[2]})} = \\underbr{A^{[2]}}{(m,n^{[2]})} - \\underbr{Y}{(m,n^{[2]})}$\n",
    "- $\\large \\underbr{dW^{[2]}}{(n^{[1]},n^{[2]})} = \\frac{1}{m} {\\underbr{A^{[1]}}{(m,n^{[1]})}}^{T} \\underbr{dZ^{[2]}}{(m,n^{[2]})} $\n",
    "- $\\large \\underbr{db^{[2]}}{(n^{[2]})} = \\frac{1}{m} \\sum_{i=1}^{m} \\underbr{dZ^{[2]}}{(m,n^{[2]})}$\n",
    "\n",
    "- $\\large \\underbr{dA^{[1]}}{(m,n^{[1]})} = \\underbr{dZ^{[2]}}{(m,n^{[2]})} {\\underbr{W^{[2]}}{(n^{[1]},n^{[2]})}}^{T} $\n",
    "- $\\large \\underbr{dZ^{[1]}}{(m,n^{[1]})} = \\underbr{dA^{[1]}}{(m,n^{[1]})} \\: \\odot \\: f' (\\underbr{Z^{[1]}}{(m,n^{[1]})})$\n",
    "- $\\large \\underbr{dW^{[1]}}{(n^{[0]},n^{[1]})} = \\frac{1}{m} {\\underbr{X}{(m,n^{[0]})}}^{T} \\underbr{dZ^{[1]}}{(m,n^{[1]})} $\n",
    "- $\\large \\underbr{db^{[1]}}{(n^{[1]})} = \\frac{1}{m} \\sum_{i=1}^{m} \\underbr{dZ^{[1]}}{(m,n^{[1]})}$\n",
    "\n",
    "\n",
    "### Backward propagation\n",
    "\n",
    "Based on the previous formulae, write the corresponding backpropagation algorithm.\n",
    "\n",
    "### Parameters update\n",
    "\n",
    "- Implement a **first version** in which the parameters are updated using a **simple gradient descent**:\n",
    "    - $W = W - \\alpha dW$\n",
    "\n",
    "\n",
    "- Implement a **second version** in which the parameters are updated using the **momentum method**:\n",
    "    - $V_{dW}(t) = \\beta V_{dW}(t-1) + (1-\\beta) dW$\n",
    "    - $W(t) = W(t-1) - \\alpha V_{dW}(t)$\n",
    "\n",
    "### IMPORTANT IMPLEMENTATION INFORMATION !\n",
    "\n",
    "The $\\odot$ operator refers to the point-wise multiplication operation. The matrix multiplication operation can be carried out in Python using ```np.dot(.,.)``` function.\n",
    "\n",
    "\n",
    "\n",
    "## Your task:\n",
    "\n",
    "You need to add the missing parts in the code (parts between ```# --- START CODE HERE``` and ```# --- END CODE HERE```)\n",
    "\n",
    "## Note \n",
    "\n",
    "The code is written as a python class (in order to be able to pass all the variables easely from one function to the other).\n",
    "\n",
    "To use a given variable, you need to use ```self.$VARIABLE_NAME```, such as````self.W1````,```self.b1```, ... (see the code already written).\n",
    "\n",
    "## Testing\n",
    "\n",
    "For testing your code, you can use the code provided in the last cells (loop over epochs and display of the loss decrease).\n",
    "You should a cost which decreases (largely) over epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-OpFxAFiopCY"
   },
   "source": [
    "# Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c05fkCgFopCY"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from sklearn import model_selection\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "student = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GzrpnPCEopCa"
   },
   "source": [
    "# Define a set of functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UDDxBp_UopCb"
   },
   "outputs": [],
   "source": [
    "def F_standardize(X):\n",
    "    \"\"\"\n",
    "    standardize X, i.e. subtract mean (over data) and divide by standard-deviation (over data)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.array of size (m, n_0)\n",
    "        matrix containing the observation data\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    X: np.array of size (m, n_0)\n",
    "        standardize version of X\n",
    "    \"\"\"\n",
    "    \n",
    "    X -= np.mean(X, axis=0, keepdims=True) \n",
    "    X /= (np.std(X, axis=0, keepdims=True) + 1e-16)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aHfEONyMopCd"
   },
   "outputs": [],
   "source": [
    "def F_sigmoid(x):\n",
    "    \"\"\"Compute the value of the sigmoid activation function\"\"\"\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def F_relu(x):\n",
    "    \"\"\"Compute the value of the Rectified Linear Unit activation function\"\"\"\n",
    "    return x * (x > 0)\n",
    "\n",
    "def F_dRelu(x):\n",
    "    \"\"\"Compute the derivative of the Rectified Linear Unit activation function\"\"\"\n",
    "    x[x<=0] = 0\n",
    "    x[x>0] = 1\n",
    "    return x\n",
    "\n",
    "def F_computeCost(hat_y, y):\n",
    "    \"\"\"Compute the cost (sum of the losses)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hat_y: (m, 1)\n",
    "        predicted value by the MLP\n",
    "    y: (m, 1)\n",
    "        ground-truth class to predict\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "     \n",
    "    if student:\n",
    "        # --- START CODE HERE (01)\n",
    "        loss =  ...\n",
    "        # --- END CODE HERE\n",
    "\n",
    "    cost = np.sum(loss) / m\n",
    "    return cost\n",
    "\n",
    "def F_computeAccuracy(hat_y, y):\n",
    "    \"\"\"Compute the accuracy\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    hat_y: (m, 1)\n",
    "        predicted value by the MLP\n",
    "    y: (m, 1)\n",
    "        ground-truth class to predict\n",
    "    \"\"\"\n",
    "    \n",
    "    m = y.shape[0]    \n",
    "    class_y = np.copy(hat_y)\n",
    "    class_y[class_y>=0.5]=1\n",
    "    class_y[class_y<0.5]=0\n",
    "    return np.sum(class_y==y) / m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "c8xKaXHlopCe"
   },
   "source": [
    "# Load dataset and pre-process it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 283
    },
    "colab_type": "code",
    "id": "kU3HU2mnopCf",
    "outputId": "3463fc5b-a5c3-4adf-f6d5-c9722d525984"
   },
   "outputs": [],
   "source": [
    "X, y = datasets.make_circles(n_samples=1000, noise=0.2, factor=0.5)\n",
    "\n",
    "from pandas import DataFrame\n",
    "# scatter plot, dots colored by class value\n",
    "df = DataFrame(dict(x=X[:,0], y=X[:,1], label=y))\n",
    "colors = {0:'red', 1:'blue'}\n",
    "fig, ax = plt.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 197
    },
    "colab_type": "code",
    "id": "xR1zNuEWopCh",
    "outputId": "e2fa410a-07db-4211-e5be-bf79e23c37d3"
   },
   "outputs": [],
   "source": [
    "print(\"X.shape: {}\".format(X.shape))\n",
    "print(\"y.shape: {}\".format(y.shape))\n",
    "print(set(y))\n",
    "\n",
    "# X is (m, n_0)\n",
    "# y is (m,)\n",
    "\n",
    "# --- Standardize data\n",
    "X = F_standardize(X)\n",
    "\n",
    "# --- Split between training set and test set\n",
    "# --- (m, n_0)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# --- Convert to proper shape: (m,) -> (m, 1)\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "y_test = y_test.reshape(len(y_test), 1)\n",
    "\n",
    "# --- Convert to oneHotEncoding: (nbExamples, 1) -> (nbExamples, nbClass)\n",
    "n_0 = X_train.shape[1]\n",
    "n_2 = 1\n",
    "\n",
    "print(\"X_train.shape: {}\".format(X_train.shape))\n",
    "print(\"X_test.shape: {}\".format(X_test.shape))\n",
    "print(\"y_train.shape: {}\".format(y_train.shape))\n",
    "print(\"y_test.shape: {}\".format(y_test.shape))\n",
    "print(\"y_train.shape: {}\".format(y_train.shape))\n",
    "print(\"y_test.shape: {}\".format(y_test.shape))\n",
    "print(\"n_0=n_in: {} n_2=n_out: {}\".format(n_0, n_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WFeLTEtqopCj"
   },
   "source": [
    "# Define the MLP class with forward, backward and update methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zMbOr4aEopCk"
   },
   "outputs": [],
   "source": [
    "class C_MultiLayerPerceptron:\n",
    "    \"\"\"\n",
    "    A class used to represent a Multi-Layer Perceptron with 1 hidden layers\n",
    "\n",
    "    ...\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    W1, b1, W2, b2:\n",
    "        weights and biases to be learnt\n",
    "    Z1, A1, Z2, A2:\n",
    "        values of the internal neurons to be used for backpropagation\n",
    "    dW1, db1, dW2, db2, dZ1, dZ2:\n",
    "        partial derivatives of the loss w.r.t. parameters\n",
    "    VdW1, Vdb1, VdW2, Vdb2:\n",
    "        momentum terms\n",
    "    do_bin0_multi1:\n",
    "        set wether we solve a binary or a multi-class classification problem\n",
    "        \n",
    "    Methods\n",
    "    -------\n",
    "    forward_propagation\n",
    "    \n",
    "    backward_propagation\n",
    "    \n",
    "    update_parameters\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    W1, b1, W2, b2 = [], [], [], []\n",
    "    A0, Z1, A1, Z2, A2 = [], [], [], [], []\n",
    "    dW1, db1, dW2, db2 = [], [], [], []   \n",
    "    dZ1, dA1, dZ2 = [], [], []\n",
    "    # --- for momentum\n",
    "    VdW1, Vdb1, VdW2, Vdb2 = [], [], [], []     \n",
    "    \n",
    "    def __init__(self, n_0, n_1, n_2):\n",
    "        self.W1 = np.random.randn(n_0, n_1) * 0.01\n",
    "        self.b1 = np.zeros(shape=(1, n_1))\n",
    "        self.W2 = np.random.randn(n_1, n_2) * 0.01\n",
    "        self.b2 = np.zeros(shape=(1, n_2))        \n",
    "        # --- for momentum\n",
    "        self.VdW1 = np.zeros(shape=(n_0, n_1)) \n",
    "        self.Vdb1 = np.zeros(shape=(1, n_1))\n",
    "        self.VdW2 = np.zeros(shape=(n_1, n_2))\n",
    "        self.Vdb2 = np.zeros(shape=(1, n_2))\n",
    "        return\n",
    "\n",
    "    \n",
    "    def __setattr__(self, attrName, val):\n",
    "        if hasattr(self, attrName):\n",
    "            self.__dict__[attrName] = val\n",
    "        else:\n",
    "            raise Exception(\"self.%s note part of the fields\" % attrName)\n",
    "\n",
    "            \n",
    "\n",
    "    def M_forwardPropagation(self, X):\n",
    "        \"\"\"Forward propagation in the MLP\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy array (nbData, nbDim)\n",
    "            observation data\n",
    "\n",
    "        Return\n",
    "        ------\n",
    "        hat_y: numpy array (nbData, 1)\n",
    "            predicted value by the MLP\n",
    "        \"\"\"\n",
    "        \n",
    "        if student:\n",
    "            # --- START CODE HERE (02)\n",
    "            self.A0 = ...\n",
    "            \n",
    "            self.Z1 = ...\n",
    "            self.A1 = ...\n",
    "            \n",
    "            self.Z2 = ...\n",
    "            self.A2 = ...\n",
    "            \n",
    "            hat_y = ...\n",
    "            # --- END CODE HERE\n",
    "        \n",
    "        return hat_y\n",
    "\n",
    "\n",
    "    def M_backwardPropagation(self, X, y):\n",
    "        \"\"\"Backward propagation in the MLP\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X: numpy array (nbData, nbDim)\n",
    "            observation data\n",
    "        y: numpy array (nbData, 1)\n",
    "            ground-truth class to predict\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        m = y.shape[0]\n",
    "        \n",
    "        if student:\n",
    "            # --- START CODE HERE (03)\n",
    "    \n",
    "            self.dZ2 = ...\n",
    "            self.dW2 = ...\n",
    "            self.db2 = ...\n",
    "            self.dA1 = ...\n",
    "\n",
    "            self.dZ1 = ...\n",
    "            self.dW1 = ...\n",
    "            self.db1 = ...\n",
    "            # --- END CODE HERE\n",
    "\n",
    "        return\n",
    "\n",
    "    \n",
    "    def M_gradientDescent(self, alpha):\n",
    "        \"\"\"Update the parameters of the network using gradient descent\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha: float scalar\n",
    "            amount of update at each step of the gradient descent\n",
    "            \n",
    "        \"\"\"\n",
    "        if student:\n",
    "            # --- START CODE HERE (04)\n",
    "            self.W1 = ...\n",
    "            self.b1 = ...\n",
    "            self.W2 = ...\n",
    "            self.b2 = ...\n",
    "            # --- END CODE HERE\n",
    "            \n",
    "        return\n",
    "\n",
    "    \n",
    "    def M_momentum(self, alpha, beta):\n",
    "        \"\"\"Update the parameters of the network using momentum method\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        alpha: float scalar\n",
    "            amount of update at each step of the gradient descent\n",
    "        beta: float scalar\n",
    "            momentum term \n",
    "        \"\"\"\n",
    "        \n",
    "        if student:\n",
    "            # --- START CODE HERE (05)\n",
    "            self.VdW1 = ...\n",
    "            self.W1 = ...\n",
    "\n",
    "            self.Vdb1 = ...\n",
    "            self.b1 = ...\n",
    "\n",
    "            self.VdW2 = ...\n",
    "            self.W2 = ...\n",
    "\n",
    "            self.Vdb2 = ...\n",
    "            self.b2 = ...\n",
    "            # --- END CODE HERE\n",
    "                \n",
    "        return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7aRPi2oQopCm"
   },
   "source": [
    "# Perform training using batch-gradiant and epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "id": "U9vEi2gsopCm",
    "outputId": "6670c902-ff2d-4bca-8e8a-62e35b58f780"
   },
   "outputs": [],
   "source": [
    "# hyper-parameters\n",
    "n_1 = 10 # number of hidden neurons\n",
    "nb_epoch = 5000 # number of epochs (number of iterations over full training set)\n",
    "alpha=0.1 # learning rate\n",
    "beta=0.9 # beat parameters for momentum\n",
    "\n",
    "\n",
    "# Instantiate the class MLP with providing \n",
    "# the size of the various layers (n_0=n_input, n_1=n_hidden, n_2=n_output) \n",
    "myMLP = C_MultiLayerPerceptron(n_0, n_1, n_2)\n",
    "\n",
    "train_cost, train_accuracy, test_cost, test_accuracy = [], [], [], []\n",
    "\n",
    "# Run over epochs\n",
    "for num_epoch in range(0, nb_epoch):\n",
    "    \n",
    "    # --- Forward\n",
    "    hat_y_train = myMLP.M_forwardPropagation(X_train)\n",
    "    \n",
    "    # --- Store results on train\n",
    "    train_cost.append( F_computeCost(hat_y_train, y_train) )\n",
    "    train_accuracy.append( F_computeAccuracy(hat_y_train, y_train) )\n",
    "    \n",
    "    # --- Backward\n",
    "    myMLP.M_backwardPropagation(X_train, y_train)\n",
    "    \n",
    "    # --- Update\n",
    "    myMLP.M_gradientDescent(alpha)\n",
    "    #myMLP.M_momentum(alpha, beta)\n",
    "\n",
    "    # --- Store results on test\n",
    "    hat_y_test = myMLP.M_forwardPropagation(X_test)\n",
    "    test_cost.append( F_computeCost(hat_y_test, y_test) )    \n",
    "    test_accuracy.append( F_computeAccuracy(hat_y_test, y_test) )\n",
    "    \n",
    "    if (num_epoch % 500)==0: \n",
    "        print(\"epoch: {0:d} (cost: train {1:.2f} test {2:.2f}) (accuracy: train {3:.2f} test {4:.2f})\".format(num_epoch, train_cost[-1], test_cost[-1], train_accuracy[-1], test_accuracy[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OWH7NblhopCo"
   },
   "source": [
    "## Display train/test loss and accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLLjoteXopCp",
    "outputId": "4367fffc-1a8b-46b2-eb9b-7731fbe8aa18"
   },
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(train_cost, 'r')\n",
    "plt.plot(test_cost, 'g--')\n",
    "plt.xlabel('# epoch')\n",
    "plt.ylabel('loss')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(train_accuracy, 'r')\n",
    "plt.plot(test_accuracy, 'g--')\n",
    "plt.xlabel('# epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0uLVa9lsopCs"
   },
   "source": [
    "# Evaluation\n",
    "\n",
    "To evaluate the work, you should rate the code for \n",
    "- 1) Loss (01)\n",
    "- 2) Forward (02)\n",
    "- 3) Backward (03)\n",
    "- 4) Parameter update by Gradient Descent (04)\n",
    "- 5) Parameter update by Momentum (05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MQYnibs_opCs"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "IA306_20192020_Lab1_MLP_python.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
