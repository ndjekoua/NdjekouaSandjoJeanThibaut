{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.2"},"colab":{"name":"TP1-avazu-blank.ipynb","provenance":[],"collapsed_sections":["1xrsFhCiYcT5","Aw0uqHqTYcVd","cF3oNg2BYcWo","lahZdWm6YcXT"]}},"cells":[{"cell_type":"markdown","metadata":{"id":"5BnGYg5FYcTb","colab_type":"text"},"source":["<h1><center>Practice of Large Scale Machine Learning - TP1 Avazu<center></h1>\n","<h2><center>IA317<center></h2>\n"]},{"cell_type":"markdown","metadata":{"id":"N2QU9Lk5YcTk","colab_type":"text"},"source":["#Introduction\n","\n","In online advertising, click-through rate (CTR) is a very important metric for evaluating ad performance. As a result, click prediction systems are essential and widely used for sponsored search and real-time bidding. The goal of this TP is to build and test prediction models on 11 days of Avazu data.\n","\n","#### For any remark or suggestion, please feel free to contact us at: \n","#### pascal.bianchi@telecom-paris.fr\n","#### nidham.gazagnadou@telecom-paris.fr\n","#### kevin.elgui@telecom-paris.fr"]},{"cell_type":"code","metadata":{"id":"z2yZusf12lgP","colab_type":"code","outputId":"271c8923-ec28-4982-9c1b-17faa8568756","executionInfo":{"status":"ok","timestamp":1574258395661,"user_tz":-60,"elapsed":21014,"user":{"displayName":"Kevin Elg","photoUrl":"","userId":"12066153934767704862"}},"colab":{"base_uri":"https://localhost:8080/","height":122}},"source":["import sys\n","import os\n","from google.colab import drive\n","\n","drive.mount('/content/gdrive')\n","root_path = 'gdrive/My Drive/Colab Notebooks/tp_avazu'  # your new root path\n","\n","sys.path.append(os.path.join(root_path, 'notebooks')) # for importing from utils.py"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n","\n","Enter your authorization code:\n","··········\n","Mounted at /content/gdrive\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"vxsbsRu7YcTo","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    Import the needed packages: numpy, pandas etc..\n","</font>"]},{"cell_type":"code","metadata":{"id":"T60UC4foYcTr","colab_type":"code","colab":{}},"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import datetime\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fz9emRwgHPNC","colab_type":"code","colab":{}},"source":["## Try to run de cell\n","from utils import plotlift"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1xrsFhCiYcT5","colab_type":"text"},"source":["# The dataset"]},{"cell_type":"markdown","metadata":{"id":"eOZWhU4lYcT9","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    Load the data *'data_tp/train-1000000'* into a pandas DataFrame. Use the function 'os.path.join()' to indicate the full path. Then, display the first lines with the \".head()\" method.\n","    <br>\n","</font>\n","**Remark**: the features meaning is available at https://www.kaggle.com/c/avazu-ctr-prediction/data"]},{"cell_type":"code","metadata":{"id":"tWvQXG4uYcUB","colab_type":"code","colab":{}},"source":["df = pd.read_csv( ... )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"uLSo2y32YcUK","colab_type":"code","colab":{}},"source":["df. ... # Fill here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Cbg9-3qRYcUQ","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    What is the 'click' frequency in the dataset?\n","    <br>\n","    Hint: 'click' is a binary output (0 or 1)\n","</font>"]},{"cell_type":"code","metadata":{"id":"l-1q2lVuYcUS","colab_type":"code","colab":{}},"source":["df['click']. ... # Fill here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OTyZBmSrKR1z","colab_type":"text"},"source":["Your answer here\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"6dFgRBOeYcUY","colab_type":"text"},"source":["We note that the Avazu dataset is very unbalanced. The 'click' class represents less than a fifth of the whole database. We already know that a classifier always predicting 'click'=0 will have good preformances in terms of error rate (around 0.17). The ROC and lift curves will be better performance metrics."]},{"cell_type":"markdown","metadata":{"id":"BFGwt6vTYcUa","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    What are the categorical features? Using the 'df.nunique()' method, compute the number of distincts values for each of these features.\n","</font>"]},{"cell_type":"code","metadata":{"id":"2eLXMTTkYcUb","colab_type":"code","colab":{}},"source":["... # Fill here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"0lirn-2rfmn_"},"source":["Your answer here\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"OGJwlJ6VYcUj","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    If one does a dummy encoding of all categorical variables, what would be the dimension of the model?\n","    In other words: what would be the dimension of our big feature vector encoded with dummies. \n","    <br>\n","    Hint: if you have just 1 feature with 3 categories A, B and C, you would have a vector of dimension 3 (or 2 if you use drop_first=True).\n","</font>"]},{"cell_type":"code","metadata":{"id":"DL2wN3tXYcUk","colab_type":"code","colab":{}},"source":["df.nunique().values ... "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4sfr6kkTfn1_"},"source":["Your answer here\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"_PYpWlX6YcUq","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    Analyze the 'hour' column: which format is used? How can we transform/simply this feature?\n","</font>"]},{"cell_type":"code","metadata":{"id":"zfbvfYBfYcUr","colab_type":"code","colab":{}},"source":["print(df[ ... ].head()) # Fill here\n","print(df[ ... ].tail()) # Fill here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Wyc_XCRmfoWk"},"source":["Your answer here\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"oTQo-GPRYcUw","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    Run and understand the following script.\n","</font>"]},{"cell_type":"code","metadata":{"id":"kOKSh2RjYcUz","colab_type":"code","colab":{}},"source":["import datetime\n","\n","def datesplit(originalDate):\n","    originalDate = str(originalDate)\n","    \n","    year = int(\"20\" + originalDate[0:2])\n","    month = int(originalDate[2:4])\n","    day = int(originalDate[4:6])\n","    hour = int(originalDate[6:8])\n","    \n","    return datetime.datetime(year, month, day, hour)\n","\n","# Exemple :\n","datesplit(14102915).weekday(), datesplit(14102915).hour"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CrRiyg01YcU4","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    Using the \".apply( ... )\" method, create a 'weekday' for the day of the week. Then, replace the 'hour' column by the hour.\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"hFhI2HFgYcU6","colab_type":"text"},"source":["To answer this question you have to understand that applies takes a function as argument\n","<br>\n","\"lambda x: ... \" is used to create local unamed function of x\n","<br>\n","Check the documentation: https://docs.python.org/3/tutorial/controlflow.html#lambda-expressions"]},{"cell_type":"code","metadata":{"id":"nDWxoncVYcU7","colab_type":"code","colab":{}},"source":["df['weekday'] = df['hour'].apply(lambda x: ... ) # Fill here\n","df['hour'] = df['hour'].apply(lambda x: ... ) # Fill here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"t7e9LOv6YcU_","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    Using the \".groupby( ... )\" method, visualize the influence of the hour and of the day on the 'click' frequency (take the \".mean()\"!). To do so, plot 'click' vs 'hour' and 'click' vs 'weekday' curves.\n","    <br>\n","    Precise the name of the axes.\n","</font>"]},{"cell_type":"code","metadata":{"id":"fLJVdQ-6YcVA","colab_type":"code","colab":{}},"source":["print(df.groupby( ... )[ ... ]) # Fill here for the influence of the hour\n","print(df.groupby( ... )[ ... ]) # Fill here for the influence of the day"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QD2kpLa-YcVF","colab_type":"code","colab":{}},"source":["plt.plot(df.groupby( ... ).mean()[ ... ]); # Fill here for the influence of the hour"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"o3fFdYYTYcVW","colab_type":"code","colab":{}},"source":["plt.plot(df.groupby( ... ).mean()[ ... ]); # Fill here for the influence of the day"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Aw0uqHqTYcVd","colab_type":"text"},"source":["# Preliminary work and first model"]},{"cell_type":"markdown","metadata":{"id":"6hlQCIDoYcVf","colab_type":"text"},"source":["Start by understanding the features with few modalities:\n","'hour', 'weekday', 'C1', 'banner_pos', 'site_category', 'app_category', 'device_type', 'device_conn_type', 'C15', 'C16', 'C18', 'C21'."]},{"cell_type":"markdown","metadata":{"id":"pl8OmuiOYcVg","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    For instance, inspect the columns 'C15' (dimension of the advert) and 'site_category': visualize the clicks mean.\n","</font>"]},{"cell_type":"code","metadata":{"id":"T0_3X-c3YcVi","colab_type":"code","colab":{}},"source":["... # Fill here: inspect 'C15' using '.groupby'"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"BjHI9kpZYcVm","colab_type":"code","colab":{}},"source":["... # Fill here: inspect 'site_category' using '.groupby'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MLfdMT-qYcVr","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    Some site categories have a null click rate. Why? Display the 'count()' of each modality.\n","    <br><br>\n","    Hint: you can use the method \".value_counts()\"\n","</font>"]},{"cell_type":"code","metadata":{"id":"0FkctWqDYcVt","colab_type":"code","colab":{}},"source":["df.[ ... ]. ... # Fill here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ru25QWd0YcVy","colab_type":"text"},"source":["It is practical to visualize both columns in parallel:"]},{"cell_type":"code","metadata":{"id":"9JwsJNMGYcV0","colab_type":"code","colab":{}},"source":["col = 'C1'\n","a = pd.DataFrame([df.groupby(col).mean()['click'], df.groupby(col).count()['click']]).transpose()\n","a.columns = ['mean', 'count']\n","a.sort_values(by='count', ascending=False)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"a07QsALahvnV"},"source":["Your answer here\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"5iAVEnE4YcV5","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    Divide the dataset into a training and test set with sklearn (use the following options: test_size=0.1, random_state=100).\n","    <br>\n","    Warning: do not use 'id' as a feature.\n","</font>"]},{"cell_type":"code","metadata":{"id":"7Mc0hmCsYcV7","colab_type":"code","colab":{}},"source":["from sklearn.model_selection import ...\n","Xtrain, Xtest, ytrain, ytest = ...."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y94Vsm-wYcV_","colab_type":"text"},"source":["# First work on a reduced number of features"]},{"cell_type":"markdown","metadata":{"id":"HffuIJthYcV_","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    Put in a list the selected columns: 'hour', 'weekday', 'C1', 'banner_pos', 'site_category', 'app_category', 'device_type', 'device_conn_type', 'C15', 'C16', 'C18', 'C21'.\n","</font>"]},{"cell_type":"code","metadata":{"id":"b87fxoxuYcWB","colab_type":"code","colab":{}},"source":["some_columns = ['hour', 'weekday', 'C1', 'banner_pos', 'site_category', 'app_category', \n","                'device_type', 'device_conn_type', 'C15', 'C16', 'C18', 'C21']"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bZQ0i2YMYcWD","colab_type":"text"},"source":["<font color=\"red\"> - Importer CategoricalEncoder de sklearn.preprocessing.  \n","- Fitter le CategoricalEncoder sur les données de train restreintes aux colonnes ci-dessus.  \n","- Transformer les données de train et de test en dummies.\n","</font>"]},{"cell_type":"markdown","metadata":{"id":"bLD-KxNcYcWF","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    <br>\n","    - Import OneHotEncoder from sklearn.preprocessing\n","    <br>\n","    - Transform the training and the test data restricted to the selected columns\n","</font>"]},{"cell_type":"code","metadata":{"id":"GDx0mpYYYcWG","colab_type":"code","colab":{}},"source":["from sklearn.preprocessing import ...\n","ohe = OneHotEncoder()\n","Xtrain_oh = ... # fit the transformation of Xtrain restricted to selected columns\n","Xtest_oh = ... # and transform Xtest restricted to selected columns"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FN5GjuyzYcWI","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    What is the new number of features?\n","</font>"]},{"cell_type":"code","metadata":{"id":"Syvv31BuYcWL","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"X2JQVVfTi4Zs"},"source":["Your answer here\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"HUWRg9VaYcWQ","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    Visualize the first row of the design matrix you have got. Do not forget to convert it into a \"numpy.array\" using the method \".toarray()\".\n","</font>"]},{"cell_type":"code","metadata":{"id":"KteEhxlwYcWR","colab_type":"code","colab":{}},"source":["Xtrain_oh[ ... ]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GcIpiLHYYcWU","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    Import and fit a logistic regression model on your the encoded data.\n","</font>"]},{"cell_type":"code","metadata":{"id":"r8a0SxavYcWV","colab_type":"code","colab":{}},"source":["from sklearn.linear_model import ...\n","lr = ...\n","lr.fit(...)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0iuQIQj-YcWZ","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    With '.predict' and 'predict_proba', display the hard and soft decision you get on test data.\n","</font>"]},{"cell_type":"code","metadata":{"id":"TYiTfHIWYcWa","colab_type":"code","colab":{}},"source":["hard = ...\n","soft = ..."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"npfzy9N3YcWc","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    Compute the probability of error using sklearn.metrics 'accuracy_score' function. Comment.\n","</font>"]},{"cell_type":"code","metadata":{"id":"hIB33gPiYcWd","colab_type":"code","colab":{}},"source":["from sklearn.metrics import accuracy_score\n","... # Fill here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Pdi72JJUoYb_"},"source":["Your answer here\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"ER4oZTUjYcWf","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    Plot the ROC curve. Then compute the log loss and the Area Under the Curve ROC.\n","</font>"]},{"cell_type":"code","metadata":{"id":"Mj-LeKtqYcWf","colab_type":"code","colab":{}},"source":["from sklearn.metrics import roc_curve\n","\n","false_pos_rate, true_pos_rate, _ = roc_curve( ... )\n","plt.plot(...)\n","\n","plt.grid()\n","plt.plot([0, 1], [0, 1], 'r--')\n","plt.xlabel('False Positive Rate', fontsize=16)\n","plt.ylabel('True Positive Rate', fontsize=16)\n","plt.title('ROC curve', fontsize=20)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RNxlXhDhYcWi","colab_type":"code","colab":{}},"source":["from sklearn.metrics import ... , ...\n","\n","log_loss( ... ), roc_auc_score( ... )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KPbreB3bYcWl","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    Import the 'plot_lift' function from 'utils.py' and plot the lift curve. What is the lift of the first fifth of the population identified by the test?\n","</font>"]},{"cell_type":"code","metadata":{"id":"0VxLpfo9YcWl","colab_type":"code","colab":{}},"source":["from utils import plotlift\n","\n","plotlift( ... )\n","# The following line just plots a vertical line for you to answer the second part of the question\n","plt.axvline(x= ... , linestyle='--', color='r') # Fill here\n","\n","plt.xlabel('Proportion of the dataset', fontsize=16)\n","plt.ylabel('Lift', fontsize=16)\n","plt.title('Lift curve', fontsize=20)\n","plt.plot()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"WM_lg75foaLV"},"source":["Your answer here\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"cF3oNg2BYcWo","colab_type":"text"},"source":["# Complete model"]},{"cell_type":"markdown","metadata":{"id":"guGisntUYcWo","colab_type":"text"},"source":["As an example, we now focus on the 'site_id' column.\n","\n","<font color=\"red\">**Question:**\n","    <br>\n","    - For each modality, compute the number of occurrences in the dataset and the average of clicks for this modality \n","    <br>\n","    - With 'sns.joiplot', represent the set of points (count, mean) for each modality\n","</font>"]},{"cell_type":"code","metadata":{"id":"EWjvCk-JYcWp","colab_type":"code","colab":{}},"source":["import seaborn as sns"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"T__2itFfYcWr","colab_type":"code","colab":{}},"source":["col = 'site_id'\n","a = pd.DataFrame([df.groupby(col).mean()['click'], df.groupby(col).count()['click']]).transpose()\n","a.columns = ['mean', 'n_val']\n","sns.jointplot(a['n_val'], a['mean'], alpha=0.25);\n","thres = 10000\n","sns.jointplot(a[a['n_val']<thres]['n_val'], a[a['n_val']<thres]['mean'], alpha=0.25);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g8_LIRbIYcWt","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    In the above commands, what does the alpha parameter stand for?\n","</font>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4BBai06xp8eh"},"source":["Your answer here\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"HNp0HTLAYcWv","colab_type":"text"},"source":["Several sites with a lot of occurrences in the dataset have a click frequency very different from the mean. It is relevant to keep the column 'site_id' at least for the modalities for which 'count' is very high.\n","\n","<font color=\"red\">**Question:**\n","    Do the same work on the 'device_id' column. What do you notice? What could be the isolated point?\n","</font>"]},{"cell_type":"code","metadata":{"id":"bLhTwP7yYcWw","colab_type":"code","colab":{}},"source":["col = ... # Fill here\n","a = pd.DataFrame([df.groupby(col).mean()['click'], df.groupby(col).count()['click']]).transpose()\n","a.columns = ['mean', 'n_val']\n","sns.jointplot(a['n_val'], a['mean'], alpha=0.25);\n","thres = 10000\n","sns.jointplot(a[a['n_val']<thres]['n_val'], a[a['n_val']<thres]['mean'], alpha=0.25);"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HxFRNmVyqI3v"},"source":["Your answer here\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"q3_evYNkYcWy","colab_type":"text"},"source":["<font color=\"red\">**Question**:   \n","    In the column df['device_id'], spot the value V corresponding to the largest number of 'count' (using '.value_counts' method). Create a new column df['user'] defined as follows:\n","</font>\n","$$\n","\\text{user} = \\left\\{\\begin{array}[h]{ll} \\text{device_ip + device_model} & \\text{if device_id = V}\\\\ \\text{device_id} & \\text{else.}\\end{array}\\right.\n","$$"]},{"cell_type":"code","metadata":{"id":"6_KH9aypYcWz","colab_type":"code","colab":{}},"source":["V = ... # Fill here\n","df['user'] = (df['device_ip'] + df['device_model']) * (df['device_id']==V) + df['device_id'] * (df['device_id']!=V)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9irHlnubYcW2","colab_type":"text"},"source":["<font color=\"red\">**Question**:\n","    Drop the following columns: 'device_id', 'device_model' and 'device_ip'\n","</font>"]},{"cell_type":"code","metadata":{"id":"0EfKNjkIYcW3","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Re0avpAtYcW7","colab_type":"text"},"source":["<font color=\"red\">**Question**:\n","    One could look at the similarity between 'site_id' and 'site_domain'. Merge those columns into a new 'site' column (using the '+' operator)  and delete the old 'site_id' and 'site_domain' columns.\n","</font>"]},{"cell_type":"code","metadata":{"id":"RPrMudxPYcW7","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qBWWV6pnYcW_","colab_type":"text"},"source":["<font color=\"red\">**Question**:\n","    Once again, split the modified DataFrame 'df' into a train and a test set (with options: test_size=0.1 and random_state=100).\n","    <br>\n","    Warning: do not use 'id' as a feature.\n","</font>"]},{"cell_type":"code","metadata":{"id":"XmJrvxlEYcXA","colab_type":"code","colab":{}},"source":["Xtrain, Xtest, ytrain, ytest = "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b7zoMXC5YcXC","colab_type":"text"},"source":["<font color=\"red\">**Question**:\n","    Define a OneHotEncoder and then, 'fit_transform' the train set.\n","</font>"]},{"cell_type":"code","metadata":{"id":"U4RSRj7LYcXC","colab_type":"code","colab":{}},"source":["ohe = OneHotEncoder()\n","Xtrain_oh = ... # Fill here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9p2yNLblYcXF","colab_type":"text"},"source":["<font color=\"red\">**Question**:\n","    Transform the test set. If an error occurs, analyze it and try to solve it by modifying the parameter \"handle_unkown\" of the OneHotEncoder. \n","    <br>\n","    Explain your the problem you encouter and how you solved it. \n","</font>"]},{"cell_type":"code","metadata":{"id":"7MeYRj1IYcXG","colab_type":"code","colab":{}},"source":["\n","Xtest_oh = ... # Fill here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"zKXtU2rBwAd-"},"source":["Your answer here\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"lrihsX2dYcXH","colab_type":"text"},"source":["<font color=\"red\">**Question**:\n","    What is the new number of features?\n","</font>"]},{"cell_type":"code","metadata":{"id":"8HF2CHhxYcXI","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"lZ9kHweWwDEt"},"source":["Your answer here\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"KTjjUA6JYcXJ","colab_type":"text"},"source":["<font color=\"red\">**Question**:\n","    How many modalities have been seen more than a 100 times?\n","</font>"]},{"cell_type":"code","metadata":{"id":"Egt0JgByYcXK","colab_type":"code","colab":{}},"source":["# The following 'n_ones' vector below gives the number of '1' in each column/modality of the Xtrain_oh design matrix.\n","n_ones = np.array(Xtrain_oh.sum(axis=0))\n","... # Fill here using 'n_ones'"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3bNDd-2dwD3_"},"source":["Your answer here\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"U0h74pNxYcXN","colab_type":"text"},"source":["The list 'cols_to_keep' below is used to store the indices of the modalities seen more than a 100 times. \n","\n","<font color=\"red\">**Question**:\n","    How does the `enumerate` function work?\n","</font>"]},{"cell_type":"code","metadata":{"id":"_YknbprTYcXO","colab_type":"code","colab":{}},"source":["useful = np.array(n_ones>100)[0]\n","cols_to_keep = [i for i,b in enumerate(useful) if b]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xMCS7vM9yjt0"},"source":["Your answer here\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"X0fiY2NBYcXP","colab_type":"text"},"source":["<font color=\"red\">**Question**:\n","    Using only our restriction on the 'cols_to_keep' columns, retrain a logistic regression model and compare its performance in terms of 'log_loss' and 'roc_auc_curve'.\n","</font>"]},{"cell_type":"code","metadata":{"id":"ajFA_JMJYcXR","colab_type":"code","colab":{}},"source":["lr = ...\n","lr.fit( ... )\n","soft = ...\n","..."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lahZdWm6YcXT","colab_type":"text"},"source":["# Gradient Boosting Decision Trees"]},{"cell_type":"markdown","metadata":{"id":"RkeyRzzmYcXU","colab_type":"text"},"source":["<font color=\"red\">**Question**:\n","    <br>\n","    - Import GradientBoostingClassifier from scikit learn. \n","    <br>\n","    - Initialize it using 50 estimators and a learning rate of 0.8. Set 'verbose=True' to monitor the progress of the training step.\n","    <br>\n","    - Fit it on the same columns 'cols_to_keep' of Xtrain_oh than for the previous logistic regression.\n","    <br>\n","    - Evaluate its performance as in the previous step.\n","</font>"]},{"cell_type":"code","metadata":{"id":"H8XZdPUXYcXV","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import ...\n","gb = ... # Fill here"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"et4JbJETYcXb","colab_type":"code","colab":{}},"source":["gb.fit( ... )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2mD8vNR-YcXd","colab_type":"code","colab":{}},"source":["soft = ...\n","log_loss(ytest, soft), roc_auc_score(ytest, soft)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LwRasRUrYcXe","colab_type":"text"},"source":["<font color=\"red\">**Question**:\n","    What does the 'gb.estimators_' output?\n","</font>"]},{"cell_type":"code","metadata":{"id":"93aySiwVYcXf","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"XR6NDWxJ3_et"},"source":["Your answer here\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"ti_dNmw8YcXi","colab_type":"text"},"source":["We now try to use the 'gb' classifier to generate features that will be relevant inputs for the logistic regression.\n","\n","<font color=\"red\">**Question**:\n","    With the 'gb.apply()' method, generate a transformation 'leafs_train' of the training set 'Xtrain_oh[:, cols_to_keep]' for which the n-th column corresponds to the number of the leaf returned by the estimator n.\n","</font>"]},{"cell_type":"code","metadata":{"id":"cmJhb7tcYcXi","colab_type":"code","colab":{}},"source":["# Test here the gb.apply method on Xtrain[:, cols_to_keep]\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f88rLF6fYcXk","colab_type":"text"},"source":["After your applied gb.apply on Xtrain_oh[:, cols_to_keep], remember that we have only one class to predict (click = 0 or 1). So, do not forget to slice your output with [:, :, 0] in order to keep, firstly, all the samples, and secondly all your 50 estimators.\n","\n","Hint: you can have a look at the dimensions of your output with .shape !"]},{"cell_type":"code","metadata":{"id":"jiZ-Q28JYcXk","colab_type":"code","colab":{}},"source":["leafs_train = pd.DataFrame( ... ) # Fill here\n","leafs_test = pd.DataFrame( ... )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qzp8_EGTYcXm","colab_type":"text"},"source":["<font color=\"red\">**Question**:\n","    Encode these new features into dummies.\n","</font>"]},{"cell_type":"code","metadata":{"id":"9NJku301YcXn","colab_type":"code","colab":{}},"source":["ohe = OneHotEncoder( ... ) # Fill here an argument if needed\n","leafs_train_oh = ...\n","leafs_test_oh = ..."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2HLSbinZYcXp","colab_type":"text"},"source":["<font color=\"red\">**Question**:\n","    With the \"hstack\" function, create a new matrix of features by concatenating horizontally our new features \"leafs_train_oh\" and \"leafs_test_oh\", and previous features used for logistic regression (e.g. in \"Xtrain_oh[:, cols_to_keep]\").\n","</font>\n","<br><br>\n","**Remark**: we use \"hstack\" from scipy.sparse (and not from numpy) because the output of our OneHotEncoder is a sparse array. You can verify it with the command \"type(Xtrain_oh)\"."]},{"cell_type":"code","metadata":{"id":"wjhhFyZvYcXp","colab_type":"code","colab":{}},"source":["from scipy.sparse import csr_matrix, hstack\n","\n","Xtrain_concat = hstack([ ... , ... ]) # Fill here\n","Xtest_concat = ..."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xU9nmnoAYcXr","colab_type":"text"},"source":["<font color=\"red\">**Question**:\n","    Run a logistic regression on the new features and evaluate its performance as before. \n","    <br>\n","    Warning: be patient, fitting should take about 5 minutes on Google Colab.\n","</font>"]},{"cell_type":"code","metadata":{"id":"GENT6fHnYcXs","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0ZtUqINfYcX2","colab_type":"text"},"source":["<font color=\"red\">**Question**:\n","    <br>\n","    - Import XGBClassifier from xgboost package.\n","    <br>\n","    - Initialize it using the same number of estimators and learning rate as before. Set the 'n_jobs' to -1.\n","    <br>\n","    - Fit it on the same columns 'cols_to_keep' of Xtrain_oh than for the previous logistic regression.\n","    <br>\n","    - Evaluate its performance and compare it to the GradientBoosting classifier of scikit learn. How is the error? Does it take more time to run?\n","</font>"]},{"cell_type":"code","metadata":{"id":"n2SeSnz_YcX3","colab_type":"code","colab":{}},"source":["from xgboost import ...\n","\n","xgb = XGBClassifier( ... , ... , ... , verbosity=2)\n","%time xgb.fit( ... )\n","..."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"55dxXIaDYcX6","colab_type":"text"},"source":["<font color=\"red\">**Question**:\n","    Increase the number of estimators and visualize the impact on performance. You might have to adapt the learning rate. On Google Colab, here are the approximate training times<br> \n","    - nb_estimators=256  and lr=0.8 ~  4 min<br>\n","    - nb_estimators=512  and lr=0.6 ~  8 min<br>\n","    - nb_estimators=1024 and lr=0.5 ~ 16 min\n","</font>"]},{"cell_type":"code","metadata":{"id":"Ggzx0320YcX7","colab_type":"code","colab":{}},"source":["xgb = XGBClassifier( ... , ... , ... )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CdGiGxdHYcX-","colab_type":"text"},"source":["# Features hashing and random forest"]},{"cell_type":"markdown","metadata":{"id":"Xl78yaijYcX_","colab_type":"text"},"source":["We restart with raw features before dummy encoding: Xtrain, Xtest.\n","\n","<font color=\"red\">**Question**:\n","    Display again the number of modalities per feature in Xtrain. Use '.nunique()'\n","</font>"]},{"cell_type":"code","metadata":{"id":"Pq6zsDXsYcYA","colab_type":"code","colab":{}},"source":["# Fill here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qIcJGDaxYcYC","colab_type":"text"},"source":["We want to define a transformation which regroups the least frequent modalities into a label 'isRare'. In order to do so, we decide to define our own Transformer.\n","\n","<font color=\"red\">**Question**:\n","    Understand the different steps of 'fit' and 'transform'.\n","</font>"]},{"cell_type":"code","metadata":{"id":"a7rJE9cSYcYC","colab_type":"code","colab":{}},"source":["from sklearn.base import BaseEstimator, TransformerMixin\n","\n","class MergeRareTransformer(BaseEstimator, TransformerMixin):\n","    \n","    def __init__(self, col_names, threshold):\n","        self.col_names = col_names\n","        self.threshold = threshold\n","\n","    def fit(self, X, y=None):\n","\n","        X = pd.DataFrame(X)\n","        counts_dict_list = []\n","        ################## READ THIS #########################\n","        for i in range(len(self.col_names)):\n","            \n","            serie = X[self.col_names[i]].value_counts()  # Série des counts de chaque modalité\n","            rare_indexes = serie[serie<self.threshold[i]].index  # A quoi correspondent ces indices ?\n","            frequent_indexes = serie[serie>=self.threshold[i]].index  # A quoi correspondent ces indices ?\n","            dico = {x:'isRare' for x in rare_indexes}\n","            dico.update({x: str(x) for x in frequent_indexes})\n","            counts_dict_list.append(dico)   # Quel est le dictionnaire obtenu ?\n","            \n","        ######################################################\n","            \n","        self.counts_dict_list_ = counts_dict_list\n","        return self\n","\n","    def transform(self, X):\n","\n","        Xt = pd.DataFrame()\n","        ################## READ THIS #########################\n","        for col, count_dict in zip(self.col_names, self.counts_dict_list_):\n","            Xt[col] = X[col].apply(lambda x:count_dict[x] if x in count_dict else 'isRare')\n","            # A quoi sert le \"else 'isRare'\" dans la transformation de la colonne ?\n","        ######################################################\n","\n","        return Xt\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"IJqAjGMkYcYE","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    <br>\n","    - Fit and transform the training set. To do so, merge all mmodalities occuring at least 20 times in each column.\n","    <br>\n","    - Transform the test set.\n","</font>"]},{"cell_type":"code","metadata":{"id":"cO4wJxgOYcYF","colab_type":"code","colab":{}},"source":["mg = MergeRareTransformer(col_names=Xtrain.columns, threshold=[20]*len(Xtrain.columns))\n","Xtrain_mg = ... # Fill here with fit_transform\n","Xtest_mg = "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"M0X1rEVJYcYI","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","   How many elements of the column Xtrain_mg['app_domain'] are now labeled as 'rare'? Use the '.value_counts()' method.\n","</font>"]},{"cell_type":"code","metadata":{"id":"5A3oL0BgYcYI","colab_type":"code","colab":{}},"source":["# Fill here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4YqrBBiSYcYK","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","   In the new train set, display the number of modalities for each feature.\n","</font>"]},{"cell_type":"code","metadata":{"id":"g_jA_EBDYcYL","colab_type":"code","colab":{}},"source":["# Fill here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0W2O7eoxYcYO","colab_type":"text"},"source":["Modalities are arbitrary object, such as '234', 'isRare', etc. We will now transorm them into integers with the hashing trick.\n","\n","<font color=\"red\">**Question:**\n","   To better understand how the 'hash' function is working, apply it to a string of your choice.\n","</font>"]},{"cell_type":"code","metadata":{"id":"4Xn6l5okYcYP","colab_type":"code","colab":{}},"source":["import hashlib\n","hash( ... ) # Fill here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m0xFTKrSYcYT","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","   Create two new datasets 'Xtrain_ha' and 'Xtest_ha' containing the output of the hashing process. Use the '.apply' method to transform iteratively the columns by mapping to each value its remainder in the euclidean division of the hash by 1000000. \n","   Example:\n","</font>\n","</font>\n","<p>\n","<center>\n","2060777048690<font color=\"red\">918393</font>  -->  918393\n","</center>\n","Remark: do not forget that the function 'hash' has to take a string as input."]},{"cell_type":"code","metadata":{"id":"Su-b4TJKYcYT","colab_type":"code","colab":{}},"source":["Xtrain_ha = pd.DataFrame()\n","Xtest_ha = pd.DataFrame()\n","for col in Xtrain_mg.columns:\n","    Xtrain_ha[col] = Xtrain_mg[col].apply(lambda ... ) # Fill here defining a lambda function\n","    Xtest_ha[col] = Xtest_mg[col].apply(lambda ... )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uvVesMxDYcYV","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","   Visualize the content of the DataFrames you have obtained.\n","</font>"]},{"cell_type":"code","metadata":{"id":"lJAHncnZYcYW","colab_type":"code","colab":{}},"source":["# Fill here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Xt7l9rvYcYX","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    <br>\n","    - Train a RandomForestClassifier on Xtrain_ha and evaluate its performance on Xtest_ha. Chose 256 estimators, min_samples_leaf=20 and verbose=1 in order to monitor the fitting step. (If you have enough time, try later with 1024 estimators and min_samples_leaf=20).\n","    <br>\n","    - How do you fix 'n_jobs'? \n","</font>"]},{"cell_type":"code","metadata":{"id":"1CtFMlwKYcYX","colab_type":"code","colab":{}},"source":["from sklearn.ensemble import ... # Fill here\n","rf = RandomForestClassifier( ... )\n","rf.fit( ... )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"h170ZelZYcYa","colab_type":"code","colab":{}},"source":["soft = ...\n","log_loss(ytest, soft), roc_auc_score(ytest, soft)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3V1PmiFGYcYb","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","   Compare with xgboost performance on the same hashed dataset, with n_estimators=256, n_jobs=-1, learning_rate=1.0. (If you have enough time try n_estimators=1024, n_jobs=-1, learning_rate=0.5, it will perform better than a Random Forest of 1024 trees).\n","</font>"]},{"cell_type":"code","metadata":{"id":"efxpo945YcYc","colab_type":"code","colab":{}},"source":["... # Fill here\n","%time xgb.fit( ..., ... ) # the '%time' command will return the execution time of the fitting step\n","..."],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sQEvD6v9YcYf","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    Plot the ROC and the lift curves for the obtained classifier (xgboost on hashed data). What is the lift of the first fifth of the population identified by the test?\n","</font>"]},{"cell_type":"code","metadata":{"id":"BTaxpNu8YcYg","colab_type":"code","colab":{}},"source":["# Plot the ROC curve here"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"t5X212VxYcYf","colab_type":"code","colab":{}},"source":["# Plot the lift curve here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"drSyzM_M8YJk"},"source":["Your answer here\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"bs1_k3-hYcYj","colab_type":"text"},"source":["Hashing produces columns of integers arbitrarily ordered. One can wonder if adding more columns hashed in a different fashion would lead to better performance. \n","\n","<font color=\"red\">**Question:**\n","    Add hashed columns by recursively hashing the previous ones: complete the following code. \n","</font>"]},{"cell_type":"code","metadata":{"id":"q2X9YoNvYcYj","colab_type":"code","colab":{}},"source":["Xtrain_haha = pd.DataFrame(Xtrain_ha).copy()\n","Xtest_haha = pd.DataFrame(Xtest_ha).copy()\n","\n","n_hash = 3\n","cols = Xtrain_ha.columns\n","for l in range(n_hash):\n","    for col in cols:\n","        Xtrain_haha[col + '-hash'] = Xtrain_haha[col].apply( ... )\n","        Xtest_haha[col + '-hash'] = Xtest_haha[col].apply( ... )\n","    cols = [col + '-hash' for col in cols]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OgdtITAfYcYq","colab_type":"text"},"source":["<font color=\"red\">**Question:**\n","    Evaluate the performance of a random forest and/or an xgboost clasifier (chose 256 estimators first and learning rate of 1.0 for xgboost) on these enlarged dataset. Is the performance better than when hasing features once?\n","</font>"]},{"cell_type":"code","metadata":{"id":"DOk1idKAYcYr","colab_type":"code","colab":{}},"source":["# Fill here"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"QW3nb8YKYcYt","colab_type":"code","colab":{}},"source":["# Fill here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"xusvruj8NWTp"},"source":["Your answer here\n","\n","..."]},{"cell_type":"markdown","metadata":{"id":"7CxxdsyAYcYu","colab_type":"text"},"source":["Good job! :-) "]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"3NTILkwg8fFQ"},"source":["<font color=\"red\">**Bonus Question:**\n","    If you have enough time (about 2 hours ...), try to fit both random forest and xgboost models with 1024 estimators (and a learning rate of 0.5 for xgboost) on these enlarged datasets.<br>\n","    - Which one performs better?<br> \n","    - Can you see the improvement due to the addition of more hashed features?\n","</font>"]},{"cell_type":"code","metadata":{"id":"MO88fPXO8-9E","colab_type":"code","colab":{}},"source":["# Fill here"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"id":"gmT_UJvZIIWT","colab_type":"text"},"source":["# Field-aware Factorization Machines\n","<br>\n","<font color=\"red\">\n","Display the names of the columns of the matrix Xtrain_ha\n","</font>"]},{"cell_type":"code","metadata":{"id":"gLVRw6AtIIWU","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jePx6DkuIIWU","colab_type":"text"},"source":["<font color=\"red\">\n","Create a list of lists of the columns. The n-th sub-list is composed of the names of the columns of the n-th field.</font>"]},{"cell_type":"code","metadata":{"id":"Xw5rYhwJIIWV","colab_type":"code","colab":{}},"source":["fields = []\n","fields.append( liste_colonnes_du_premier_champ )\n","fields.append( liste_colonnes_du_deuxième_champ )\n","fields.append( etc. )\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZUu2FXaSIIWW","colab_type":"text"},"source":["<font color=\"red\">\n","With the enumerate function, let generate a dictionary which associates each column name with its field.\n"]},{"cell_type":"code","metadata":{"id":"tWzYMksxIIWX","colab_type":"code","colab":{}},"source":["dicFields = { ... : ...  for i,f in enumerate(fields) for name_col in f}"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"o2Scxx_CIIWZ","colab_type":"text"},"source":["We need to make sure that two different columns have different modalities. For this, we add in front of each modality a characteristic string of the column.\n","\n","<font color=\"red\">Perform this transformation with the apply method</font>\n"]},{"cell_type":"code","metadata":{"id":"Y253v6YBIIWa","colab_type":"code","colab":{}},"source":["Xtrain_ffm = pd.DataFrame()\n","Xtest_ffm = pd.DataFrame()\n","for col in Xtrain_ha.columns:\n","    Xtrain_ffm[col] = Xtrain_ha[col].apply( ... )\n","    Xtest_ffm[col] = Xtest_ha[col].apply( ... )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yrhsnGjIIIWb","colab_type":"text"},"source":["With np.unique, we create the list of all the modalities appearing in all the columns."]},{"cell_type":"code","metadata":{"id":"7fcTcK4rIIWb","colab_type":"code","colab":{}},"source":["allvals = []\n","allcols_ffm = [col for f in fields for col in f]  # Toutes les colonnes appartenant à l'un des champ\n","for col in allcols_ffm:\n","    allvals.append(np.unique(Xtrain_ffm[col]))\n","allvals= [val for sublist in allvals for val in sublist]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OBLCa95zIIWc","colab_type":"text"},"source":["<font color=\"red\">\n","Using enumerate method, create a dictionary that at each value associates its rank in the allvals list.</font>"]},{"cell_type":"code","metadata":{"id":"TlcO1vu0IIWc","colab_type":"code","colab":{}},"source":["dicFeat = {val:i for i,val in ... }"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qS4cP1SgIIWd","colab_type":"text"},"source":["We build dataframe from dicFeat as follows:\n","in each \"col\" column, the value \"x\" is replaced by the string \"f: v: 1\", where f is the field associated with the column and v is the index of the modality x.\n","<font color=\"red\">\n","Study the following code</font>"]},{"cell_type":"code","metadata":{"id":"FLXawWuYIIWd","colab_type":"code","colab":{}},"source":["for col in allcols_ffm:\n","    f = dicFields[col]\n","    Xtrain_ffm[col] = Xtrain_ffm[col].apply(lambda x:\"{}:{}:1\".format(f,dicFeat[x]) if x in dicFeat else \"\")\n","    Xtest_ffm[col] = Xtest_ffm[col].apply(lambda x:\"{}:{}:1\".format(f,dicFeat[x]) if x in dicFeat else \"\")"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O8jnU8x8IIWe","colab_type":"text"},"source":["<font color=\"red\">For the two resulting dataframes, concatenate on the left the corresponding ytrain (or ytest) click column</font>"]},{"cell_type":"code","metadata":{"id":"6vVXBrtVIIWe","colab_type":"code","colab":{}},"source":["yXtrain_ffm = pd.concat( ... ,axis=1)\n","yXtest_ffm = pd.concat( ... ,axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2yKP8AdSIIWf","colab_type":"text"},"source":["<font color=\"red\">What does the following cell produce?</font>"]},{"cell_type":"code","metadata":{"id":"vlCgqefuIIWg","colab_type":"code","colab":{}},"source":["train_w = yXtrain_ffm.apply(lambda row:' '.join(row.values),axis=1).values\n","test_w = yXtest_ffm.apply(lambda row:' '.join(row.values),axis=1).values"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7VI0cKgcIIWm","colab_type":"text"},"source":["<font color=\"red\">Write the above dataframe in a text file</font>"]},{"cell_type":"code","metadata":{"id":"fdJFgYQuIIWo","colab_type":"code","colab":{}},"source":["thefile = open('train.txt','w')\n","for item in train_w:\n","    thefile.write(\"%s\\n\" % item)\n","\n","# Faire de même pour le test set\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z3-j2EGuIIWo","colab_type":"text"},"source":["To learn the FFM model, we use the LibFFM library https://www.csie.ntu.edu.tw/~cjlin/libffm/\n","In the notebooks / directory there are two executables: ffm-train and ffm-predict.\n","In jupyter notebook, the exclamation point! allows to launch a bash command.\n","The use is as follows:\n","\n","-   `ffm-train'\n","\n","    usage: ffm-train [options] training_set_file [model_file]\n","\n","    options:  \n","    -l <lambda>: set regularization parameter (default 0.00002)  \n","    -k <factor>: set number of latent factors (default 4)  \n","    -t <iteration>: set number of iterations (default 15)  \n","    -r <eta>: set learning rate (default 0.2)  \n","    -s <nr_threads>: set number of threads (default 1)  \n","    -p <path>: set path to the validation set  \n","    --quiet: quiet model (no output)  \n","    --no-norm: disable instance-wise normalization  \n","    --auto-stop: stop at the iteration that achieves the best validation loss (must be used with -p)  \n","<br>\n","<font color=\"red\">Learn the model. We can :\n","- use an regularizer of 0.00001\n","- use a learning rate of 0.05\n","- use of the order of 15 latent factors\n","- carry out the order of 30 iterations\n","- with the -p option, change the test file to validation parameters\n","- activate the --autostop mode\n","</font> "]},{"cell_type":"code","metadata":{"id":"zgFB_Wz7IIWp","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ksgNKsj8IIWs","colab_type":"text"},"source":["The use of ffm-predict is: ffm-predict filename-test file-name-model file-in-which-write-the-prediction \n","<font color=\"red\">Perform the prediction</font>"]},{"cell_type":"code","metadata":{"id":"8J6KBW8OIIWs","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pp02SsNnIIWt","colab_type":"text"},"source":["\n","\n","```\n","# Ce texte est au format code\n","```\n","\n","<font color=\"red\">Compute the log_loss and area under the roc curve (ROC AUC).</font>"]},{"cell_type":"code","metadata":{"id":"nFFrqJyZIIWu","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"colab_type":"text","id":"vqR8fOOBOqxx"},"source":["# Blending"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"7-3iP_FwOqxw"},"source":["<font color=\"red\">\n","Split Xtrain in two: Xtrain_meth and Xtrain_blend.</font>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VOz5fHUgOqxr","colab":{}},"source":["Xtrain_meth, Xtrain_blend, ytrain_meth, ytrain_blend = train_test_split(Xtrain,ytrain,test_size=0.2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"collapsed":true,"colab_type":"text","id":"taRGY6UDOqxq"},"source":["<font color=\"red\">\n","Train a logistic regression on XTrain_meth. Then compute its score on Xtrain_blend and save the score in a vector.</font>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"iLZ9eMbEOqxo","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"dENR18QvOqxn"},"source":["<font color=\"red\">\n","Do the same thing with XGBoost (save the score).</font>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8H4Z6I86Oqxl","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"b8voXiEwOqxk"},"source":["<font color=\"red\">\n","Do the same with a RandomForest (save the score).</font>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"At6NVRKZOqxi","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"iJcPtHjLOqxh"},"source":["<font color=\"red\">\n","Now with an extraTrees.</font>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"3GVkx2YjOqxf","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wUy2BfQSOqxe"},"source":["<font color=\"red\">\n","And again with FFM.</font>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"PmSahlQCOqxb","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"Jk56lAPTOqxZ"},"source":["<font color=\"red\">\n","Concatenate all scores on Xtrain_blend to form a Zblend design matrix.</font>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"y4Fz0wSLOqxT","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"px1CkkL7OqxR"},"source":["<font color=\"red\">\n","Train a logistic regression on (Zblend,ytrain_blend).</font>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"awqpZTPhOqxN","colab":{}},"source":[""],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nzWT4vH-OqxJ"},"source":["<font color=\"red\">\n","Evaluate the performance of this model on the test dataset.</font>"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zq_IriMnOqw6","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}